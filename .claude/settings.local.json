{
  "permissions": {
    "allow": [
      "Bash(C:/Users/jkw88/miniconda3/envs/volumequant/python.exe -c \"import sys; sys.path.insert\\(0, ''C:/dev/claude-code/eps-momentum-us''\\); from daily_runner import analyze_technical; print\\(analyze_technical\\(''AAPL''\\)\\)\")",
      "Bash(set PYTHONIOENCODING=utf-8)",
      "Bash(C:/Users/jkw88/miniconda3/envs/volumequant/python.exe -c \"import sys; sys.path.insert\\(0, ''C:/dev/claude-code/eps-momentum-us''\\); from daily_runner import analyze_technical; result = analyze_technical\\(''AAPL''\\); print\\(result\\)\")",
      "Bash(C:/Users/jkw88/miniconda3/envs/volumequant/python.exe:*)",
      "Bash(git add:*)",
      "Bash(git commit:*)",
      "Bash(git push:*)",
      "Bash(schtasks /query:*)",
      "Bash(schtasks /query /tn \"*EPS*\")",
      "Bash(schtasks /create:*)",
      "Bash(timeout:*)",
      "Bash(powershell -command \"Get-Content ''C:\\\\Users\\\\jkw88\\\\AppData\\\\Local\\\\Temp\\\\claude\\\\C--dev-claude-code-eps-momentum-us\\\\tasks\\\\b71327e.output'' -Tail 30 2>$null\")",
      "Bash(git remote add:*)",
      "Bash(python -c:*)",
      "Bash(\"C:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\volumequant\\\\python.exe\" --version)",
      "Bash(\"C:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\volumequant\\\\python.exe\":*)",
      "Bash(\"C:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\volumequant\\\\python.exe\" -m pip install googletrans==4.0.0-rc1 --quiet)",
      "Bash(\"C:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\volumequant\\\\python.exe\" -c \"from googletrans import Translator; t = Translator\\(\\); print\\(''googletrans OK''\\)\")",
      "Bash(\"C:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\volumequant\\\\python.exe\" daily_runner.py)",
      "Bash(\"C:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\volumequant\\\\python.exe\" -u daily_runner.py)",
      "Bash(\"C:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\volumequant\\\\python.exe\" -c:*)",
      "Bash(\"C:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\volumequant\\\\python.exe\" -c \"\nimport requests\nimport json\nimport os\n\nos.chdir\\(r''C:\\\\dev\\\\claude code\\\\eps-momentum-us''\\)\n\nBOT_TOKEN = ''7948087946:AAGVHj7FdBxr0LRJzQTzfEp0HadzAtoXs-8''\nCHAT_ID = ''7580571403''\n\n# Message 1: Market Status\nmarket_msg = ''''''[시장 상태] 2026-02-05\n\nNASDAQ: 22,905 \\(-1.51%\\)\n  MA50: 23,384\n  상태: 하락 \\(MA50 아래\\)\n\nS&P500: 6,883 \\(-0.51%\\)  \n  MA50: 6,877\n  상태: 보합 \\(MA50 근접\\)\n\nVIX: 18.64 \\(정상 범위\\)\n\n[시장 판정: RED]\n이유: 나스닥이 50일 이동평균선 아래\n-> 가장 엄격한 필터 적용\n-> Score >= 8.0, PEG < 1.0\n\n결과: 917개 중 37개 통과 \\(4.0%\\)\n''''''\n\nurl = f''https://api.telegram.org/bot{BOT_TOKEN}/sendMessage''\nresp = requests.post\\(url, json={''chat_id'': CHAT_ID, ''text'': market_msg}\\)\nprint\\(f''Market msg: {resp.status_code}''\\)\n\")",
      "Bash(\"C:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\volumequant\\\\python.exe\" \"C:\\\\dev\\\\claude code\\\\eps-momentum-us\\\\send_11_37.py\")",
      "Bash(\"C:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\volumequant\\\\python.exe\" \"C:\\\\dev\\\\claude code\\\\eps-momentum-us\\\\send_summary.py\")",
      "Bash(\"C:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\volumequant\\\\python.exe\" \"C:\\\\dev\\\\claude code\\\\eps-momentum-us\\\\send_formatted.py\")",
      "Bash(\"C:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\volumequant\\\\python.exe\" \"C:\\\\dev\\\\claude code\\\\eps-momentum-us\\\\send_11_37_formatted.py\")",
      "Bash(\"C:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\volumequant\\\\python.exe\" \"C:\\\\dev\\\\claude code\\\\eps-momentum-us\\\\send_final_top10.py\")",
      "Bash(\"C:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\volumequant\\\\python.exe\" \"C:\\\\dev\\\\claude code\\\\eps-momentum-us\\\\send_final_11_37.py\")",
      "Bash(\"C:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\volumequant\\\\python.exe\" \"C:\\\\dev\\\\claude code\\\\eps-momentum-us\\\\send_final_11_37_v2.py\")",
      "Bash(\"C:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\volumequant\\\\python.exe\" \"C:\\\\dev\\\\claude code\\\\eps-momentum-us\\\\send_11_37_split.py\")",
      "Bash(\"C:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\volumequant\\\\python.exe\" \"C:\\\\dev\\\\claude code\\\\eps-momentum-us\\\\send_beautiful_11_25.py\")",
      "Bash(\"C:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\volumequant\\\\python.exe\" \"C:\\\\dev\\\\claude code\\\\eps-momentum-us\\\\send_beautiful_26_37.py\")",
      "Bash(\"C:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\volumequant\\\\python.exe\" \"C:\\\\dev\\\\claude code\\\\eps-momentum-us\\\\send_final_format.py\")",
      "Bash(\"C:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\volumequant\\\\python.exe\" \"C:\\\\dev\\\\claude code\\\\eps-momentum-us\\\\send_final_v3.py\")",
      "Bash(\"C:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\volumequant\\\\python.exe\" \"C:\\\\dev\\\\claude code\\\\eps-momentum-us\\\\send_top10_final.py\")",
      "Bash(\"C:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\volumequant\\\\python.exe\" \"C:\\\\dev\\\\claude code\\\\eps-momentum-us\\\\send_26_37_v3.py\")",
      "Bash(\"C:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\volumequant\\\\python.exe\" -c \"\nimport pandas as pd\ndf = pd.read_csv\\(r''C:\\\\dev\\\\claude code\\\\eps-momentum-us\\\\eps_data\\\\screening_2026-02-05.csv''\\)\n\nprint\\(''현재 통과 종목: 37개''\\)\nprint\\(\\)\n\n# 새 필터 적용\ndf[''margin_improving''] = df[''op_growth''] > df[''rev_growth'']\ndf[''rev_10_plus''] = df[''rev_growth''] >= 10\n\n# 기존 통과 종목 중 새 필터 통과 여부\nprint\\(''=== TOP 10 필터 적용 결과 ===''\\)\ntop10 = df.head\\(10\\)[[''ticker'', ''rev_growth'', ''op_growth'', ''rev_10_plus'', ''margin_improving'']]\ntop10[''통과''] = \\(top10[''rev_10_plus'']\\) & \\(top10[''margin_improving'']\\)\nprint\\(top10.to_string\\(\\)\\)\n\nprint\\(\\)\nprint\\(''=== 전체 37개 중 새 필터 통과 ===''\\)\npassed = df[\\(df[''rev_growth''] >= 10\\) & \\(df[''op_growth''] > df[''rev_growth'']\\)]\nprint\\(f''통과: {len\\(passed\\)}개''\\)\nprint\\(\\)\nprint\\(''탈락 종목:''\\)\nfailed = df[~\\(\\(df[''rev_growth''] >= 10\\) & \\(df[''op_growth''] > df[''rev_growth'']\\)\\)]\nprint\\(failed[[''ticker'', ''rev_growth'', ''op_growth'']].to_string\\(\\)\\)\n\")",
      "Bash(\"C:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\volumequant\\\\python.exe\" -c \"\nimport pandas as pd\ndf = pd.read_csv\\(r''C:\\\\dev\\\\claude code\\\\eps-momentum-us\\\\eps_data\\\\screening_2026-02-05.csv''\\)\n\nprint\\(''=== EPS 관련 컬럼 ===''\\)\neps_cols = [c for c in df.columns if ''eps'' in c.lower\\(\\) or ''current'' in c.lower\\(\\) or ''d'' in c.lower\\(\\) and ''score'' not in c.lower\\(\\)]\nprint\\([c for c in df.columns if any\\(x in c for x in [''7d'', ''30d'', ''60d'', ''90d'', ''current'', ''aligned'', ''score_321'', ''score_slope'']\\)]\\)\n\nprint\\(\\)\nprint\\(''=== TOP 10 EPS 기간별 데이터 ===''\\)\nprint\\(df[[''ticker'', ''current'', ''7d'', ''30d'', ''60d'', ''90d'', ''is_aligned'', ''score_321'', ''eps_chg_60d'']].head\\(10\\).to_string\\(\\)\\)\n\")",
      "Bash(\"C:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\volumequant\\\\python.exe\" -c \"\nimport pandas as pd\nimport sys\nsys.path.insert\\(0, r''C:\\\\dev\\\\claude code\\\\eps-momentum-us''\\)\n\nfrom eps_momentum_system import calculate_quality_score, calculate_value_score\n\ndf = pd.read_csv\\(r''C:\\\\dev\\\\claude code\\\\eps-momentum-us\\\\eps_data\\\\screening_2026-02-05.csv''\\)\n\n# 1. 성장 필터 \\(v7.1 강화\\)\ndf[''growth_filter''] = \\(df[''rev_growth''] >= 10\\) & \\(df[''op_growth''] > df[''rev_growth'']\\)\nfiltered = df[df[''growth_filter'']].copy\\(\\)\n\nprint\\(f''성장 필터 통과: {len\\(filtered\\)}개 \\(기존 37개\\)''\\)\nprint\\(\\)\n\n# 2. 새 품질 점수 계산\ndef calc_new_quality\\(row\\):\n    eps_chg_7d = \\(row[''current''] - row[''7d'']\\) / row[''7d''] * 100 if row[''7d''] else 0\n    eps_chg_30d = \\(row[''current''] - row[''30d'']\\) / row[''30d''] * 100 if row[''30d''] else 0\n    eps_chg_60d = \\(row[''current''] - row[''60d'']\\) / row[''60d''] * 100 if row[''60d''] else 0\n    eps_chg_90d = \\(row[''current''] - row[''90d'']\\) / row[''90d''] * 100 if row[''90d''] else 0\n    \n    score, grade = calculate_quality_score\\(\n        is_aligned=row[''is_aligned''],\n        roe=row.get\\(''roe'', 0\\),\n        eps_chg=row.get\\(''eps_chg_60d'', 0\\),\n        above_ma200=row.get\\(''price'', 0\\) > row.get\\(''ma_200'', 0\\) if row.get\\(''ma_200''\\) else False,\n        volume_spike=row.get\\(''volume_spike'', False\\),\n        momentum_score=row.get\\(''score_321'', 0\\),\n        eps_chg_7d=eps_chg_7d,\n        eps_chg_30d=eps_chg_30d,\n        eps_chg_60d=eps_chg_60d,\n        eps_chg_90d=eps_chg_90d\n    \\)\n    return score, grade\n\n# 3. 새 가격 점수 계산\ndef calc_new_price\\(row\\):\n    score, label = calculate_value_score\\(\n        peg=row.get\\(''peg_calculated''\\),\n        fwd_per=row.get\\(''fwd_per''\\),\n        from_52w_high=row.get\\(''from_52w_high''\\),\n        rsi=row.get\\(''rsi''\\),\n        volume_spike=row.get\\(''volume_spike'', False\\)\n    \\)\n    return score, label\n\nfiltered[''new_quality''], filtered[''new_quality_grade''] = zip\\(*filtered.apply\\(calc_new_quality, axis=1\\)\\)\nfiltered[''new_price''], filtered[''new_price_label''] = zip\\(*filtered.apply\\(calc_new_price, axis=1\\)\\)\nfiltered[''new_total''] = filtered[''new_quality''] + filtered[''new_price'']\n\n# 정렬\nresult = filtered.sort_values\\(''new_total'', ascending=False\\)\n\nprint\\(''=== v7.1 새 점수 체계 TOP 15 ===''\\)\nprint\\(\\)\ncols = [''ticker'', ''new_quality'', ''new_quality_grade'', ''new_price'', ''new_price_label'', ''new_total'']\nprint\\(result[cols].head\\(15\\).round\\(1\\).to_string\\(\\)\\)\n\nprint\\(\\)\nprint\\(''=== 기존 vs 새 점수 비교 ===''\\)\ncols2 = [''ticker'', ''quality_score'', ''new_quality'', ''value_score'', ''new_price'', ''actionable_score_v63'', ''new_total'']\nprint\\(result[cols2].head\\(10\\).round\\(1\\).to_string\\(\\)\\)\n\")",
      "Bash(\"C:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\volumequant\\\\python.exe\" \"C:\\\\dev\\\\claude code\\\\eps-momentum-us\\\\send_v71_top10.py\")",
      "Bash(\"C:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\volumequant\\\\python.exe\" \"C:\\\\dev\\\\claude code\\\\eps-momentum-us\\\\send_v71_11_26.py\")",
      "Bash(\"C:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\volumequant\\\\python.exe\" -c \"\nimport sqlite3\nconn = sqlite3.connect\\(''eps_momentum_data.db''\\)\ncursor = conn.cursor\\(\\)\n\n# TOP 26 종목의 실제 가격 조회\ntickers = [''CMC'', ''LRCX'', ''WDC'', ''LITE'', ''NEM'', ''MU'', ''AVGO'', ''STX'', ''FIVE'', ''RGLD'',\n           ''GMED'', ''AMGN'', ''FTI'', ''JBL'', ''HII'', ''INCY'', ''TPR'', ''DRI'', ''LLY'', ''CCL'',\n           ''CVNA'', ''RMD'', ''CBOE'', ''CAH'', ''DGX'', ''ROK'']\n\nfor ticker in tickers:\n    cursor.execute\\(''SELECT price FROM screening WHERE ticker = ? ORDER BY date DESC LIMIT 1'', \\(ticker,\\)\\)\n    row = cursor.fetchone\\(\\)\n    if row:\n        print\\(f''{ticker}: {row[0]:.2f}''\\)\n    else:\n        print\\(f''{ticker}: NOT FOUND''\\)\n\nconn.close\\(\\)\n\")",
      "Bash(\"C:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\volumequant\\\\python.exe\" -c \"\nimport sqlite3\nconn = sqlite3.connect\\(''eps_momentum_data.db''\\)\ncursor = conn.cursor\\(\\)\ncursor.execute\\(\"\"SELECT name FROM sqlite_master WHERE type=''table''\"\"\\)\nprint\\(''Tables:'', [row[0] for row in cursor.fetchall\\(\\)]\\)\nconn.close\\(\\)\n\")",
      "Bash(\"C:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\volumequant\\\\python.exe\" -c \"\nimport sqlite3\nconn = sqlite3.connect\\(''eps_momentum_data.db''\\)\ncursor = conn.cursor\\(\\)\ncursor.execute\\(''PRAGMA table_info\\(eps_snapshots\\)''\\)\nprint\\(''Columns:'', [row[1] for row in cursor.fetchall\\(\\)]\\)\nconn.close\\(\\)\n\")",
      "Bash(\"C:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\volumequant\\\\python.exe\" \"C:\\\\dev\\\\claude code\\\\eps-momentum-us\\\\send_v71_final.py\")",
      "Bash(\"C:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\volumequant\\\\python.exe\" -c \"\nimport sqlite3\nimport yfinance as yf\n\nconn = sqlite3.connect\\(''eps_momentum_data.db''\\)\ncursor = conn.cursor\\(\\)\n\n# TOP 26 종목 티커\ntickers = [''CMC'', ''LRCX'', ''WDC'', ''LITE'', ''NEM'', ''MU'', ''AVGO'', ''STX'', ''FIVE'', ''RGLD'',\n           ''GMED'', ''AMGN'', ''FTI'', ''JBL'', ''HII'', ''INCY'', ''TPR'', ''DRI'', ''LLY'', ''CCL'',\n           ''CVNA'', ''RMD'', ''CBOE'', ''CAH'', ''DGX'', ''ROK'']\n\n# DB에서 가격과 품질/가격 점수 조회\nfor ticker in tickers[:10]:\n    cursor.execute\\(''''''\n        SELECT price, hybrid_score, rsi, from_52w_high \n        FROM eps_snapshots \n        WHERE ticker = ? \n        ORDER BY date DESC LIMIT 1\n    '''''', \\(ticker,\\)\\)\n    row = cursor.fetchone\\(\\)\n    if row:\n        print\\(f''{ticker}: price={row[0]:.2f}, hybrid={row[1]}, rsi={row[2]}, 52w={row[3]}''\\)\n\nconn.close\\(\\)\n\n# yfinance로 실제 등락률 확인\nprint\\(''\\\\n--- yfinance 등락률 ---''\\)\nfor ticker in tickers[:10]:\n    try:\n        stock = yf.Ticker\\(ticker\\)\n        hist = stock.history\\(period=''5d''\\)\n        if len\\(hist\\) >= 2:\n            prev_close = hist[''Close''].iloc[-2]\n            last_close = hist[''Close''].iloc[-1]\n            change_pct = \\(last_close - prev_close\\) / prev_close * 100\n            print\\(f''{ticker}: {last_close:.2f} \\({change_pct:+.2f}%\\)''\\)\n    except Exception as e:\n        print\\(f''{ticker}: Error - {e}''\\)\n\")",
      "Bash(\"C:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\volumequant\\\\python.exe\" -c \"\nprint\\(''=== Kill Switch 테스트 ===''\\)\nprint\\(\\)\n\n# Kill Switch 기준: current < d7 * 0.99 이면 탈락\n\ntest_cases = [\n    \\(''정상 \\(상승\\)'', 110, 100, True\\),\n    \\(''정상 \\(유지\\)'', 100, 100, True\\),\n    \\(''경계 \\(-0.5%\\)'', 99.5, 100, True\\),\n    \\(''경계 \\(-1.0%\\)'', 99.0, 100, True\\),\n    \\(''탈락 \\(-1.1%\\)'', 98.9, 100, False\\),\n    \\(''탈락 \\(-5%\\)'', 95, 100, False\\),\n]\n\nfor name, current, d7, expected_pass in test_cases:\n    threshold = d7 * 0.99\n    actual_pass = current >= threshold\n    status = ''PASS'' if actual_pass else ''KILL''\n    match = ''✓'' if actual_pass == expected_pass else ''✗''\n    print\\(f''{name}: Current={current}, 7d={d7}, 기준={threshold:.1f} → {status} {match}''\\)\n\nprint\\(\\)\nprint\\(''Kill Switch 기준: EPS\\(Current\\) < EPS\\(7d\\) × 0.99 이면 탈락''\\)\nprint\\(''= 7일 전 대비 1% 이상 하락 시 제외''\\)\n\")",
      "Bash(\"C:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\volumequant\\\\python.exe\" -c \"\nprint\\(''=== Kill Switch Test ===''\\)\nprint\\(\\)\n\ntest_cases = [\n    \\(''Normal \\(+10%%\\)'', 110, 100, True\\),\n    \\(''Normal \\(0%%\\)'', 100, 100, True\\),\n    \\(''Edge \\(-0.5%%\\)'', 99.5, 100, True\\),\n    \\(''Edge \\(-1.0%%\\)'', 99.0, 100, True\\),\n    \\(''KILL \\(-1.1%%\\)'', 98.9, 100, False\\),\n    \\(''KILL \\(-5%%\\)'', 95, 100, False\\),\n]\n\nfor name, current, d7, expected_pass in test_cases:\n    threshold = d7 * 0.99\n    actual_pass = current >= threshold\n    status = ''PASS'' if actual_pass else ''KILL''\n    match = ''OK'' if actual_pass == expected_pass else ''ERR''\n    print\\(f''{name}: Current={current}, 7d={d7}, Threshold={threshold:.1f} -> {status} [{match}]''\\)\n\nprint\\(\\)\nprint\\(''Kill Switch: EPS\\(Current\\) < EPS\\(7d\\) x 0.99 = KILL''\\)\nprint\\(''= 7d -1%% drop -> excluded''\\)\n\")",
      "Bash(\"C:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\volumequant\\\\python.exe\" -c \"\nfrom eps_momentum_system import calculate_quality_score, calculate_value_score\n\nprint\\(''=== v7.1 \\(100+100 -> 50%+50% = 100\\) ===''\\)\nprint\\(\\)\n\n# LRCX 유사\nq1, g1 = calculate_quality_score\\(\n    is_aligned=True, roe=66, eps_chg=30, above_ma200=True, volume_spike=False, momentum_score=10,\n    eps_chg_7d=15, eps_chg_30d=20, eps_chg_60d=25, eps_chg_90d=30\n\\)\nv1, l1 = calculate_value_score\\(peg=1.5, fwd_per=20, from_52w_high=-17, rsi=50, volume_spike=False\\)\ntotal1 = \\(q1 * 0.5\\) + \\(v1 * 0.5\\)\nprint\\(f''LRCX: Value {q1:.0f}/100 + Price {v1:.0f}/100''\\)\nprint\\(f''      Total = {q1:.0f}x50%% + {v1:.0f}x50%% = {total1:.1f}''\\)\nprint\\(\\)\n\n# AVGO 유사\nq2, g2 = calculate_quality_score\\(\n    is_aligned=False, roe=30, eps_chg=10, above_ma200=False, volume_spike=True, momentum_score=5,\n    eps_chg_7d=5, eps_chg_30d=8, eps_chg_60d=10, eps_chg_90d=12\n\\)\nv2, l2 = calculate_value_score\\(peg=2.0, fwd_per=25, from_52w_high=-26, rsi=31, volume_spike=True\\)\ntotal2 = \\(q2 * 0.5\\) + \\(v2 * 0.5\\)\nprint\\(f''AVGO: Value {q2:.0f}/100 + Price {v2:.0f}/100''\\)\nprint\\(f''      Total = {q2:.0f}x50%% + {v2:.0f}x50%% = {total2:.1f}''\\)\nprint\\(\\)\n\n# CMC 유사\nq3, g3 = calculate_quality_score\\(\n    is_aligned=True, roe=40, eps_chg=20, above_ma200=True, volume_spike=True, momentum_score=8,\n    eps_chg_7d=12, eps_chg_30d=18, eps_chg_60d=22, eps_chg_90d=25\n\\)\nv3, l3 = calculate_value_score\\(peg=0.3, fwd_per=10, from_52w_high=-1.7, rsi=72, volume_spike=True\\)\ntotal3 = \\(q3 * 0.5\\) + \\(v3 * 0.5\\)\nprint\\(f''CMC: Value {q3:.0f}/100 + Price {v3:.0f}/100''\\)\nprint\\(f''     Total = {q3:.0f}x50%% + {v3:.0f}x50%% = {total3:.1f}''\\)\n\")",
      "Bash(python:*)",
      "Bash(py:*)",
      "Bash(tasklist:*)",
      "Bash(findstr:*)",
      "Bash(del:*)",
      "Bash(where:*)",
      "Bash(dir \"C:\\\\dev\\\\volumequant\\\\venv\\\\Scripts\\\\python.exe\")",
      "Bash(dir:*)",
      "Bash(C:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\volumequant\\\\python.exe:*)",
      "Bash(\"C:/Users/user/miniconda3/envs/volumequant/python.exe\" \"C:/dev/claude code/eps-momentum-us/daily_runner.py\")",
      "Bash(\"C:/Users/user/miniconda3/envs/volumequant/python.exe\" -c \"\nimport sys\nsys.path.insert\\(0, r''C:/dev/claude code/eps-momentum-us''\\)\nimport pandas as pd\nimport json\nfrom daily_runner import generate_telegram_message_v71, send_telegram_message\n\n# Load existing CSV\ndf = pd.read_csv\\(r''C:/dev/claude code/eps-momentum-us/eps_data/screening_2026-02-05.csv''\\)\n\n# Load config\nwith open\\(r''C:/dev/claude code/eps-momentum-us/config.json'', ''r''\\) as f:\n    config = json.load\\(f\\)\n\n# Sample stats\nstats = {\n    ''total'': 917,\n    ''market_regime'': {\n        ''regime'': ''RED'',\n        ''ndx_price'': 22905,\n        ''ndx_ma50'': 23384,\n        ''spx_price'': 6061,\n        ''vix'': 16.5\n    }\n}\n\n# Generate message\nmessages = generate_telegram_message_v71\\(df, stats\\)\n\n# Print first message structure preview\nprint\\(''=== Message 1 Preview \\(first 2000 chars\\) ===''\\)\nprint\\(messages[0][:2000]\\)\nprint\\(''...''\\)\nprint\\(\\)\nprint\\(''=== Sending to Telegram ===''\\)\nfor i, msg in enumerate\\(messages\\):\n    result = send_telegram_message\\(config, msg\\)\n    print\\(f''Message {i+1}: {result}''\\)\n\")",
      "Bash(\"C:/Users/user/miniconda3/envs/volumequant/python.exe\":*)",
      "Bash(\"C:/Users/user/miniconda3/envs/volumequant/python.exe\" -c:*)",
      "Bash(\"C:/Users/user/miniconda3/envs/volumequant/python.exe\" -c \"\nimport sys\nsys.path.insert\\(0, r''C:/dev/claude code/eps-momentum-us''\\)\nimport pandas as pd\nimport json\nfrom daily_runner import create_telegram_message_v71, send_telegram\n\ndf = pd.read_csv\\(r''C:/dev/claude code/eps-momentum-us/eps_data/screening_2026-02-05.csv''\\)\nwith open\\(r''C:/dev/claude code/eps-momentum-us/config.json'', ''r'', encoding=''utf-8''\\) as f:\n    config = json.load\\(f\\)\n\nstats = {\n    ''total'': 917,\n    ''market_regime'': {''regime'': ''RED'', ''ndx_price'': 22905, ''ndx_ma50'': 23384, ''spx_price'': 6061, ''vix'': 16.5}\n}\n\nmessages = create_telegram_message_v71\\(df, stats\\)\n\nprint\\(''=== Sending ===''\\)\nfor i, m in enumerate\\(messages\\):\n    result = send_telegram\\(m, config\\)\n    print\\(f''Message {i+1}: {result}''\\)\n\")",
      "Bash(\"C:/Users/user/miniconda3/envs/volumequant/python.exe\" -c \"\nimport pandas as pd\ndf = pd.read_csv\\(r''C:/dev/claude code/eps-momentum-us/eps_data/screening_2026-02-05.csv''\\)\nprint\\(''=== sector 컬럼 고유값 ===''\\)\nprint\\(df[''sector''].value_counts\\(\\)\\)\n\")",
      "Bash(\"C:/Users/user/miniconda3/envs/volumequant/python.exe\" -c \"\nimport sys\nsys.path.insert\\(0, r''C:/dev/claude code/eps-momentum-us''\\)\nimport pandas as pd\nimport json\nfrom daily_runner import create_telegram_message_v71, send_telegram\n\ndf = pd.read_csv\\(r''C:/dev/claude code/eps-momentum-us/eps_data/screening_2026-02-05.csv''\\)\nwith open\\(r''C:/dev/claude code/eps-momentum-us/config.json'', ''r'', encoding=''utf-8''\\) as f:\n    config = json.load\\(f\\)\n\nstats = {\n    ''total'': 917,\n    ''market_regime'': {''regime'': ''RED'', ''ndx_price'': 22905, ''ndx_ma50'': 23384, ''spx_price'': 6061, ''vix'': 16.5}\n}\n\nmessages = create_telegram_message_v71\\(df, stats\\)\n\n# 1-2위 리스크 확인\nmsg = messages[0]\nstart = msg.find\\(''1위''\\)\nend = msg.find\\(''3위''\\)\nprint\\(''=== 1-2위 리스크 확인 ===''\\)\nprint\\(msg[start:end]\\)\n\nprint\\(''=== Sending ===''\\)\nfor i, m in enumerate\\(messages\\):\n    result = send_telegram\\(m, config\\)\n    print\\(f''Message {i+1}: {result}''\\)\n\")",
      "Bash(\"C:/Users/user/miniconda3/envs/volumequant/python.exe\" -c \"\nimport sys\nsys.path.insert\\(0, ''C:/dev/claude code/eps-momentum-us''\\)\nfrom daily_runner import get_display_dates\n\ndates = get_display_dates\\(\\)\nprint\\(''=== 날짜 테스트 ===''\\)\nprint\\(f\"\"인사말 \\(한국날짜\\): {dates[''kr_date'']}\"\"\\)\nprint\\(f\"\"시장데이터 \\(미국 영업일\\): {dates[''us_date'']}\"\"\\)\nprint\\(f\"\"미국 영업일 \\(짧은형식\\): {dates[''us_date_short'']}\"\"\\)\nprint\\(f\"\"미국 영업일 \\(ISO\\): {dates[''us_date_iso'']}\"\"\\)\n\")",
      "Bash(git status:*)",
      "Bash(\"C:/Users/user/miniconda3/envs/volumequant/python.exe\" -c \"\nimport pandas as pd\ndf = pd.read_csv\\(''C:/dev/claude code/eps-momentum-us/eps_data/screening_2026-02-05.csv''\\)\nprint\\(f''전체 종목 수: {len\\(df\\)}''\\)\nprint\\(\\)\nprint\\(''=== industry 컬럼 분포 ===''\\)\nif ''industry'' in df.columns:\n    print\\(df[''industry''].value_counts\\(\\)\\)\nelse:\n    print\\(''industry 컬럼 없음''\\)\nprint\\(\\)\nprint\\(''=== sector 컬럼 분포 ===''\\)\nif ''sector'' in df.columns:\n    print\\(df[''sector''].value_counts\\(\\)\\)\n\")",
      "Bash(git pull:*)",
      "Bash(git stash:*)",
      "Bash(git stash pop:*)",
      "Bash(\"C:/Users/user/miniconda3/envs/volumequant/python.exe\" -c \"\nimport urllib.request\nimport urllib.parse\n\nbot_token = ''7948087946:AAGVHj7FdBxr0LRJzQTzfEp0HadzAtoXs-8''\nprivate_id = ''7580571403''\n\nurl = f''https://api.telegram.org/bot{bot_token}/sendMessage''\ndata = urllib.parse.urlencode\\({\n    ''chat_id'': private_id,\n    ''text'': ''테스트 메시지입니다. 로컬에서 봇에만 전송되는지 확인용.'',\n    ''parse_mode'': ''HTML''\n}\\).encode\\(\\)\n\nreq = urllib.request.Request\\(url, data=data\\)\nresponse = urllib.request.urlopen\\(req, timeout=10\\)\nprint\\(''전송 성공!'' if response.status == 200 else ''실패''\\)\n\")",
      "Bash(\"C:/Users/user/miniconda3/envs/volumequant/python.exe\" -c \"\nimport sqlite3\nconn = sqlite3.connect\\(''C:/dev/claude code/eps-momentum-us/eps_momentum_data.db''\\)\ncursor = conn.cursor\\(\\)\ncursor.execute\\(''SELECT MIN\\(date\\), MAX\\(date\\), COUNT\\(DISTINCT date\\) FROM eps_snapshots''\\)\nmin_date, max_date, count = cursor.fetchone\\(\\)\nprint\\(f''시작일: {min_date}''\\)\nprint\\(f''종료일: {max_date}''\\)\nprint\\(f''누적 일수: {count}일''\\)\nconn.close\\(\\)\n\")",
      "Bash(sqlite3:*)",
      "Bash(find:*)",
      "Bash(grep:*)",
      "WebSearch",
      "Bash(C:Usersjkw88miniconda3envsvolumequantpython.exe daily_runner.py)",
      "Bash(xargs ls:*)",
      "Bash(sort:*)",
      "Bash(python3:*)",
      "Bash(git rm:*)",
      "Bash(conda env list:*)",
      "Bash(\"C:\\\\Users\\\\jkw88\\\\miniconda3\\\\envs\\\\volumequant\\\\python.exe\" -c \"import sys; print\\(sys.executable\\); print\\(sys.version\\)\")",
      "Bash(\"C:\\\\Users\\\\jkw88\\\\miniconda3\\\\envs\\\\volumequant\\\\python.exe\" -c:*)",
      "Bash(\"C:\\\\Users\\\\jkw88\\\\miniconda3\\\\envs\\\\volumequant\\\\python.exe\" -c \"\nimport sys\nsys.path.insert\\(0, r''C:\\\\dev\\\\claude-code\\\\eps-momentum-us''\\)\nfrom daily_runner import load_config\nfrom eps_momentum_system import INDICES, INDUSTRY_MAP, calculate_ntm_eps, calculate_ntm_score, get_trend_lights\nprint\\(''All imports successful''\\)\nprint\\(f''INDICES count: {sum\\(len\\(v\\) for v in INDICES.values\\(\\)\\)}''\\)\nprint\\(f''INDUSTRY_MAP count: {len\\(INDUSTRY_MAP\\)}''\\)\nprint\\(f''Python: {sys.executable}''\\)\n\")",
      "Bash(C:\\\\Users\\\\jkw88\\\\miniconda3\\\\envs\\\\volumequant\\\\python.exe /tmp/analyze_screening.py)",
      "Bash(/mnt/c/Users/jkw88/miniconda3/envs/volumequant/python.exe:*)",
      "Bash(cmd.exe /c \"where python\")",
      "Bash(/c/Users/jkw88/miniconda3/envs/volumequant/python.exe:*)",
      "Bash(/tmp/analyze_screening.py:*)",
      "Bash(C:Usersjkw88miniconda3envsvolumequantpython.exe -c \"import google.generativeai; print\\(google.generativeai.__version__\\)\")",
      "Bash(\"C:\\\\Users\\\\jkw88\\\\miniconda3\\\\envs\\\\volumequant\\\\python.exe\" -c \"import google.generativeai; print\\(google.generativeai.__version__\\)\")",
      "Bash(\"C:\\\\Users\\\\jkw88\\\\miniconda3\\\\envs\\\\volumequant\\\\python.exe\" -m pip install google-genai)",
      "WebFetch(domain:ai.google.dev)",
      "Bash(\"C:\\\\Users\\\\jkw88\\\\miniconda3\\\\envs\\\\volumequant\\\\python.exe\":*)",
      "Bash(powershell -Command:*)",
      "Bash(\"C:\\\\Users\\\\jkw88\\\\miniconda3\\\\envs\\\\volumequant\\\\python.exe\" -c \"\nimport sys, os, re\nsys.path.insert\\(0, r''C:\\\\dev\\\\claude-code\\\\eps-momentum-us''\\)\nfrom daily_runner import load_config, run_ai_analysis, send_telegram_long, log\n\n# 이전 실행에서 저장된 메시지 파일 사용 대신, 간단한 더미 메시지로 테스트\n# 실제 full runner의 AI 분석만 재실행\n\nconfig = load_config\\(\\)\n\n# 최근 full run에서 생성된 Part1/2/TA 메시지를 재현하기 어려우므로\n# DB에서 메시지 재생성 \\(가격 데이터 없이\\)\nfrom pathlib import Path\nimport sqlite3, pandas as pd, json\nfrom eps_momentum_system import calculate_ntm_score, get_trend_lights\n\nDB_PATH = Path\\(r''C:\\\\dev\\\\claude-code\\\\eps-momentum-us''\\) / ''eps_momentum_data.db''\nconn = sqlite3.connect\\(str\\(DB_PATH\\)\\)\ntoday_date = pd.read_sql\\(''SELECT MAX\\(date\\) as d FROM ntm_screening'', conn\\)[''d''].iloc[0]\ndf_main = pd.read_sql\\(f\"\"SELECT * FROM ntm_screening WHERE date=''{today_date}'' AND is_turnaround=0 ORDER BY rank\"\", conn\\)\ndf_ta = pd.read_sql\\(f\"\"SELECT * FROM ntm_screening WHERE date=''{today_date}'' AND is_turnaround=1\"\", conn\\)\nconn.close\\(\\)\n\ncache = {}\ncp = os.path.join\\(r''C:\\\\dev\\\\claude-code\\\\eps-momentum-us'', ''ticker_info_cache.json''\\)\nwith open\\(cp, ''r'', encoding=''utf-8''\\) as f:\n    cache = json.load\\(f\\)\n\nfor df in [df_main, df_ta]:\n    for idx, row in df.iterrows\\(\\):\n        t = row[''ticker'']\n        if t in cache:\n            df.at[idx, ''short_name''] = cache[t].get\\(''shortName'', t\\)\n            df.at[idx, ''industry''] = cache[t].get\\(''industry'', ''''\\)\n        ntm = {''current'': row[''ntm_current''], ''7d'': row[''ntm_7d''], ''30d'': row[''ntm_30d''], ''60d'': row[''ntm_60d''], ''90d'': row[''ntm_90d'']}\n        score_val, s1, s2, s3, s4, _ = calculate_ntm_score\\(ntm\\)\n        lights, desc = get_trend_lights\\(s1, s2, s3, s4\\)\n        df.at[idx, ''trend_lights''] = lights\n        df.at[idx, ''trend_desc''] = desc\n        df.at[idx, ''ntm_cur''] = row[''ntm_current'']\n\n# Part 1만 직접 생성 \\(Part 2는 가격 데이터 필요하므로 간략 대체\\)\nfrom daily_runner import create_part1_message, create_turnaround_message\nmsg_part1 = create_part1_message\\(df_main\\)\ndf_ta = df_ta.sort_values\\(''score'', ascending=False\\).reset_index\\(drop=True\\)\nmsg_turnaround = create_turnaround_message\\(df_ta\\)\n\n# Part 2 대체 \\(가격 데이터 없으므로 Part 1 상위를 참조로\\)\nmsg_part2 = ''\\(Part 2: 가격 데이터 미포함 - Part 1 상위 종목 참조\\)''\n\nprint\\(''AI 분석 호출 중...''\\)\nresult = run_ai_analysis\\(msg_part1, msg_part2, msg_turnaround, config\\)\n\nif result:\n    print\\(f''분석 완료: {len\\(result\\)}자''\\)\n    private_id = config.get\\(''telegram_private_id''\\) or config.get\\(''telegram_chat_id''\\)\n    success = send_telegram_long\\(result, config, chat_id=private_id\\)\n    print\\(f''전송 {\"\"성공\"\" if success else \"\"실패\"\"}''\\)\n    print\\(\\)\n    print\\(result[:500]\\)\nelse:\n    print\\(''AI 분석 실패''\\)\n\")",
      "Bash(chcp:*)",
      "Bash(\"C:\\\\Users\\\\jkw88\\\\miniconda3\\\\envs\\\\volumequant\\\\python.exe\" -c \"\nimport sys, os, sqlite3, pandas as pd, io\nsys.stdout = io.TextIOWrapper\\(sys.stdout.buffer, encoding=''utf-8''\\)\nsys.path.insert\\(0, r''C:\\\\dev\\\\claude-code\\\\eps-momentum-us''\\)\nfrom eps_momentum_system import calculate_ntm_score, get_trend_lights\n\nconn = sqlite3.connect\\(r''C:\\\\dev\\\\claude-code\\\\eps-momentum-us\\\\eps_momentum_data.db''\\)\ntoday = pd.read_sql\\(''SELECT MAX\\(date\\) as d FROM ntm_screening'', conn\\)[''d''].iloc[0]\n\nrow = pd.read_sql\\(f\"\"SELECT * FROM ntm_screening WHERE date=''{today}'' AND ticker=''MSTR''\"\", conn\\).iloc[0]\nntm = {''current'': row[''ntm_current''], ''7d'': row[''ntm_7d''], ''30d'': row[''ntm_30d''], ''60d'': row[''ntm_60d''], ''90d'': row[''ntm_90d'']}\nscore, s1, s2, s3, s4, is_ta = calculate_ntm_score\\(ntm\\)\nlights, desc = get_trend_lights\\(s1, s2, s3, s4\\)\n\nprint\\(''=== MSTR ===''\\)\nprint\\(f''NTM: 90d={row[\"\"ntm_90d\"\"]:.2f} -> 60d={row[\"\"ntm_60d\"\"]:.2f} -> 30d={row[\"\"ntm_30d\"\"]:.2f} -> 7d={row[\"\"ntm_7d\"\"]:.2f} -> now={row[\"\"ntm_current\"\"]:.2f}''\\)\nprint\\(f''seg4\\(90d-60d\\)={s4:+.1f}%  seg3\\(60d-30d\\)={s3:+.1f}%  seg2\\(30d-7d\\)={s2:+.1f}%  seg1\\(7d-now\\)={s1:+.1f}%''\\)\nprint\\(f''Score = {s4:.1f} + {s3:.1f} + {s2:.1f} + \\({s1:.1f}\\) = {score:.1f}''\\)\nprint\\(f''Lights: {lights} \\({desc}\\)''\\)\nprint\\(\\)\n\n# Top 5 comparison\nprint\\(''=== Top 5 comparison ===''\\)\ntop5 = pd.read_sql\\(f\"\"SELECT * FROM ntm_screening WHERE date=''{today}'' AND is_turnaround=0 ORDER BY rank LIMIT 5\"\", conn\\)\nfor _, r in top5.iterrows\\(\\):\n    n = {''current'': r[''ntm_current''], ''7d'': r[''ntm_7d''], ''30d'': r[''ntm_30d''], ''60d'': r[''ntm_60d''], ''90d'': r[''ntm_90d'']}\n    sc, a, b, c, d, _ = calculate_ntm_score\\(n\\)\n    l, ds = get_trend_lights\\(a, b, c, d\\)\n    print\\(f''{int\\(r[\"\"rank\"\"]\\):2d}. {r[\"\"ticker\"\"]:6s}  score={sc:6.1f}  s4={d:+6.1f} s3={c:+6.1f} s2={b:+6.1f} s1={a:+6.1f}  {l} {ds}''\\)\n\nconn.close\\(\\)\n\")",
      "Bash(\"C:\\\\Users\\\\jkw88\\\\miniconda3\\\\envs\\\\volumequant\\\\python.exe\" -c \"\nimport sys, sqlite3, pandas as pd\nsys.path.insert\\(0, r''C:\\\\dev\\\\claude-code\\\\eps-momentum-us''\\)\nfrom eps_momentum_system import calculate_ntm_score, get_trend_lights\n\nconn = sqlite3.connect\\(r''C:\\\\dev\\\\claude-code\\\\eps-momentum-us\\\\eps_momentum_data.db''\\)\ntoday = pd.read_sql\\(''SELECT MAX\\(date\\) as d FROM ntm_screening'', conn\\)[''d''].iloc[0]\n\nrow = pd.read_sql\\(f\"\"SELECT * FROM ntm_screening WHERE date=''{today}'' AND ticker=''MSTR''\"\", conn\\).iloc[0]\nntm = {''current'': row[''ntm_current''], ''7d'': row[''ntm_7d''], ''30d'': row[''ntm_30d''], ''60d'': row[''ntm_60d''], ''90d'': row[''ntm_90d'']}\nscore, s1, s2, s3, s4, is_ta = calculate_ntm_score\\(ntm\\)\nlights, desc = get_trend_lights\\(s1, s2, s3, s4\\)\n\nprint\\(''=== MSTR ===''\\)\nprint\\(f''NTM: 90d={row[\"\"ntm_90d\"\"]:.2f} -> 60d={row[\"\"ntm_60d\"\"]:.2f} -> 30d={row[\"\"ntm_30d\"\"]:.2f} -> 7d={row[\"\"ntm_7d\"\"]:.2f} -> now={row[\"\"ntm_current\"\"]:.2f}''\\)\nprint\\(f''seg4\\(90d-60d\\)={s4:+.1f}%  seg3\\(60d-30d\\)={s3:+.1f}%  seg2\\(30d-7d\\)={s2:+.1f}%  seg1\\(7d-now\\)={s1:+.1f}%''\\)\nprint\\(f''Score = {s4:.1f} + {s3:.1f} + {s2:.1f} + \\({s1:.1f}\\) = {score:.1f}''\\)\nprint\\(f''Lights: {lights} \\({desc}\\)''\\)\nprint\\(\\)\n\n# Top 5 comparison\nprint\\(''=== Top 5 ===''\\)\ntop5 = pd.read_sql\\(f\"\"SELECT * FROM ntm_screening WHERE date=''{today}'' AND is_turnaround=0 ORDER BY rank LIMIT 5\"\", conn\\)\nfor _, r in top5.iterrows\\(\\):\n    n = {''current'': r[''ntm_current''], ''7d'': r[''ntm_7d''], ''30d'': r[''ntm_30d''], ''60d'': r[''ntm_60d''], ''90d'': r[''ntm_90d'']}\n    sc, a, b, c, d, _ = calculate_ntm_score\\(n\\)\n    l, ds = get_trend_lights\\(a, b, c, d\\)\n    print\\(f''{int\\(r[\"\"rank\"\"]\\):2d}. {r[\"\"ticker\"\"]:6s}  score={sc:6.1f}  s4={d:+6.1f} s3={c:+6.1f} s2={b:+6.1f} s1={a:+6.1f}  {l} {ds}''\\)\n\nconn.close\\(\\)\n\")",
      "Bash(\"C:\\\\Users\\\\jkw88\\\\miniconda3\\\\envs\\\\volumequant\\\\python.exe\" -c \"\nimport sys, sqlite3, pandas as pd\nsys.path.insert\\(0, r''C:\\\\dev\\\\claude-code\\\\eps-momentum-us''\\)\nfrom eps_momentum_system import calculate_ntm_score\n\nconn = sqlite3.connect\\(r''C:\\\\dev\\\\claude-code\\\\eps-momentum-us\\\\eps_momentum_data.db''\\)\ntoday = pd.read_sql\\(''SELECT MAX\\(date\\) as d FROM ntm_screening'', conn\\)[''d''].iloc[0]\ndf = pd.read_sql\\(f\"\"SELECT * FROM ntm_screening WHERE date=''{today}'' AND is_turnaround=0\"\", conn\\)\n\nall_segs = []\nfor _, r in df.iterrows\\(\\):\n    n = {''current'': r[''ntm_current''], ''7d'': r[''ntm_7d''], ''30d'': r[''ntm_30d''], ''60d'': r[''ntm_60d''], ''90d'': r[''ntm_90d'']}\n    sc, s1, s2, s3, s4, _ = calculate_ntm_score\\(n\\)\n    all_segs.extend\\([s1, s2, s3, s4]\\)\n\nsegs = pd.Series\\(all_segs\\)\nprint\\(f''Total segments: {len\\(segs\\)} \\(from {len\\(df\\)} stocks x 4\\)''\\)\nprint\\(\\)\nprint\\(''=== Positive segment distribution ===''\\)\npos = segs[segs > 0]\nprint\\(f''  0-0.5%:   {len\\(segs[\\(segs>=0\\)&\\(segs<0.5\\)]\\):4d}  \\({len\\(segs[\\(segs>=0\\)&\\(segs<0.5\\)]\\)/len\\(segs\\)*100:.1f}%\\)''\\)\nprint\\(f''  0.5-2%:   {len\\(segs[\\(segs>=0.5\\)&\\(segs<2\\)]\\):4d}  \\({len\\(segs[\\(segs>=0.5\\)&\\(segs<2\\)]\\)/len\\(segs\\)*100:.1f}%\\)''\\)\nprint\\(f''  2-10%:    {len\\(segs[\\(segs>=2\\)&\\(segs<10\\)]\\):4d}  \\({len\\(segs[\\(segs>=2\\)&\\(segs<10\\)]\\)/len\\(segs\\)*100:.1f}%\\)''\\)\nprint\\(f''  10-20%:   {len\\(segs[\\(segs>=10\\)&\\(segs<20\\)]\\):4d}  \\({len\\(segs[\\(segs>=10\\)&\\(segs<20\\)]\\)/len\\(segs\\)*100:.1f}%\\)''\\)\nprint\\(f''  20-50%:   {len\\(segs[\\(segs>=20\\)&\\(segs<50\\)]\\):4d}  \\({len\\(segs[\\(segs>=20\\)&\\(segs<50\\)]\\)/len\\(segs\\)*100:.1f}%\\)''\\)\nprint\\(f''  >50%:     {len\\(segs[segs>=50]\\):4d}  \\({len\\(segs[segs>=50]\\)/len\\(segs\\)*100:.1f}%\\)''\\)\nprint\\(\\)\nprint\\(''=== Negative segment distribution ===''\\)\nprint\\(f''  0 to -2%: {len\\(segs[\\(segs<0\\)&\\(segs>=-2\\)]\\):4d}  \\({len\\(segs[\\(segs<0\\)&\\(segs>=-2\\)]\\)/len\\(segs\\)*100:.1f}%\\)''\\)\nprint\\(f''  -2 to -5%:{len\\(segs[\\(segs<-2\\)&\\(segs>=-5\\)]\\):4d}  \\({len\\(segs[\\(segs<-2\\)&\\(segs>=-5\\)]\\)/len\\(segs\\)*100:.1f}%\\)''\\)\nprint\\(f''  -5 to-10%:{len\\(segs[\\(segs<-5\\)&\\(segs>=-10\\)]\\):4d}  \\({len\\(segs[\\(segs<-5\\)&\\(segs>=-10\\)]\\)/len\\(segs\\)*100:.1f}%\\)''\\)\nprint\\(f''  <-10%:    {len\\(segs[segs<-10]\\):4d}  \\({len\\(segs[segs<-10]\\)/len\\(segs\\)*100:.1f}%\\)''\\)\n\nconn.close\\(\\)\n\")",
      "Bash(\"C:\\\\Users\\\\jkw88\\\\miniconda3\\\\envs\\\\volumequant\\\\python.exe\" -c \"\nimport sys, os, json, sqlite3\nimport pandas as pd\nsys.path.insert\\(0, r''C:\\\\dev\\\\claude-code\\\\eps-momentum-us''\\)\nfrom daily_runner import load_config, create_part1_message, send_telegram_long\nfrom eps_momentum_system import calculate_ntm_score, get_trend_lights\n\nconfig = load_config\\(\\)\nconn = sqlite3.connect\\(r''C:\\\\dev\\\\claude-code\\\\eps-momentum-us\\\\eps_momentum_data.db''\\)\ntoday = pd.read_sql\\(''SELECT MAX\\(date\\) as d FROM ntm_screening'', conn\\)[''d''].iloc[0]\ndf = pd.read_sql\\(f\"\"SELECT * FROM ntm_screening WHERE date=''{today}'' AND is_turnaround=0 ORDER BY rank\"\", conn\\)\nconn.close\\(\\)\n\ncache = {}\ncp = os.path.join\\(r''C:\\\\dev\\\\claude-code\\\\eps-momentum-us'', ''ticker_info_cache.json''\\)\nwith open\\(cp, ''r'', encoding=''utf-8''\\) as f:\n    cache = json.load\\(f\\)\n\nfor idx, row in df.iterrows\\(\\):\n    t = row[''ticker'']\n    if t in cache:\n        df.at[idx, ''short_name''] = cache[t].get\\(''shortName'', t\\)\n        df.at[idx, ''industry''] = cache[t].get\\(''industry'', ''''\\)\n    ntm = {''current'': row[''ntm_current''], ''7d'': row[''ntm_7d''], ''30d'': row[''ntm_30d''], ''60d'': row[''ntm_60d''], ''90d'': row[''ntm_90d'']}\n    score_val, s1, s2, s3, s4, _ = calculate_ntm_score\\(ntm\\)\n    lights, desc = get_trend_lights\\(s1, s2, s3, s4\\)\n    df.at[idx, ''trend_lights''] = lights\n    df.at[idx, ''trend_desc''] = desc\n\nmsg = create_part1_message\\(df\\)\nprint\\(f''Message length: {len\\(msg\\)} chars''\\)\n\nprivate_id = config.get\\(''telegram_private_id''\\) or config.get\\(''telegram_chat_id''\\)\nsuccess = send_telegram_long\\(msg, config, chat_id=private_id\\)\nprint\\(f''Send: {\"\"OK\"\" if success else \"\"FAIL\"\"}''\\)\n\")",
      "Bash(\"C:\\\\Users\\\\jkw88\\\\miniconda3\\\\envs\\\\volumequant\\\\python.exe\" -c \"\nimport sys, os, json, sqlite3\nimport pandas as pd\nimport yfinance as yf\nsys.path.insert\\(0, r''C:\\\\dev\\\\claude-code\\\\eps-momentum-us''\\)\nfrom daily_runner import load_config, create_part1_message, create_part2_message, send_telegram_long\nfrom eps_momentum_system import calculate_ntm_score, get_trend_lights\n\nconfig = load_config\\(\\)\nconn = sqlite3.connect\\(r''C:\\\\dev\\\\claude-code\\\\eps-momentum-us\\\\eps_momentum_data.db''\\)\ntoday = pd.read_sql\\(''SELECT MAX\\(date\\) as d FROM ntm_screening'', conn\\)[''d''].iloc[0]\ndf = pd.read_sql\\(f\"\"SELECT * FROM ntm_screening WHERE date=''{today}'' AND is_turnaround=0 ORDER BY rank\"\", conn\\)\nconn.close\\(\\)\n\ncache = {}\ncp = os.path.join\\(r''C:\\\\dev\\\\claude-code\\\\eps-momentum-us'', ''ticker_info_cache.json''\\)\nwith open\\(cp, ''r'', encoding=''utf-8''\\) as f:\n    cache = json.load\\(f\\)\n\nfor idx, row in df.iterrows\\(\\):\n    t = row[''ticker'']\n    if t in cache:\n        df.at[idx, ''short_name''] = cache[t].get\\(''shortName'', t\\)\n        df.at[idx, ''industry''] = cache[t].get\\(''industry'', ''''\\)\n    ntm = {''current'': row[''ntm_current''], ''7d'': row[''ntm_7d''], ''30d'': row[''ntm_30d''], ''60d'': row[''ntm_60d''], ''90d'': row[''ntm_90d'']}\n    score_val, s1, s2, s3, s4, _ = calculate_ntm_score\\(ntm\\)\n    lights, desc = get_trend_lights\\(s1, s2, s3, s4\\)\n    df.at[idx, ''trend_lights''] = lights\n    df.at[idx, ''trend_desc''] = desc\n    df.at[idx, ''ntm_cur''] = row[''ntm_current'']\n\n# Part 1\nmsg1 = create_part1_message\\(df\\)\nprint\\(f''Part 1: {len\\(msg1\\)} chars''\\)\n\n# Price data for Part 2\nprint\\(''Downloading prices...''\\)\ntickers_top50 = df.head\\(50\\)[''ticker''].tolist\\(\\)\nprice_data = yf.download\\(tickers_top50, period=''100d'', progress=False\\)\nfor idx, row in df.head\\(50\\).iterrows\\(\\):\n    t = row[''ticker'']\n    nc = row[''ntm_current'']\n    n7, n30, n60, n90 = row[''ntm_7d''], row[''ntm_30d''], row[''ntm_60d''], row[''ntm_90d'']\n    if n90 != 0:\n        df.at[idx, ''eps_change_90d''] = \\(nc - n90\\) / abs\\(n90\\) * 100\n    else:\n        df.at[idx, ''eps_change_90d''] = 0\n    segs = []\n    for newer, older in [\\(nc, n7\\), \\(n7, n30\\), \\(n30, n60\\), \\(n60, n90\\)]:\n        segs.append\\(\\(newer - older\\) / abs\\(older\\) * 100 if older != 0 else 0\\)\n    df.at[idx, ''eps_chg_weighted''] = segs[0]*0.4 + segs[1]*0.3 + segs[2]*0.2 + segs[3]*0.1\n    try:\n        if t in price_data[''Close''].columns:\n            prices = price_data[''Close''][t].dropna\\(\\)\n            if len\\(prices\\) >= 2:\n                cur = prices.iloc[-1]\n                p7 = prices.iloc[-6] if len\\(prices\\) >= 6 else prices.iloc[0]\n                p30 = prices.iloc[-23] if len\\(prices\\) >= 23 else prices.iloc[0]\n                p60 = prices.iloc[-46] if len\\(prices\\) >= 46 else prices.iloc[0]\n                p90 = prices.iloc[-69] if len\\(prices\\) >= 69 else prices.iloc[0]\n                pc = [\\(cur/p7-1\\)*100, \\(cur/p30-1\\)*100, \\(cur/p60-1\\)*100, \\(cur/p90-1\\)*100]\n                df.at[idx, ''price_chg_weighted''] = pc[0]*0.4 + pc[1]*0.3 + pc[2]*0.2 + pc[3]*0.1\n                if nc > 0:\n                    df.at[idx, ''fwd_pe''] = cur / nc\n                    if n90 > 0:\n                        df.at[idx, ''fwd_pe_chg''] = \\(cur/nc - p90/n90\\)\n    except:\n        pass\n\nmsg2 = create_part2_message\\(df\\)\nprint\\(f''Part 2: {len\\(msg2\\) if msg2 else 0} chars''\\)\n\nprivate_id = config.get\\(''telegram_private_id''\\) or config.get\\(''telegram_chat_id''\\)\ns1 = send_telegram_long\\(msg1, config, chat_id=private_id\\)\ns2 = send_telegram_long\\(msg2, config, chat_id=private_id\\) if msg2 else False\nprint\\(f''Part 1: {\"\"OK\"\" if s1 else \"\"FAIL\"\"}, Part 2: {\"\"OK\"\" if s2 else \"\"FAIL\"\"}''\\)\n\")",
      "Bash(\"C:\\\\Users\\\\jkw88\\\\miniconda3\\\\envs\\\\volumequant\\\\python.exe\" -c \"\nimport sys, os, json, sqlite3\nimport pandas as pd\nimport yfinance as yf\nsys.path.insert\\(0, r''C:\\\\dev\\\\claude-code\\\\eps-momentum-us''\\)\nfrom daily_runner import load_config, create_part1_message, create_part2_message, run_ai_analysis, send_telegram_long\nfrom eps_momentum_system import calculate_ntm_score, get_trend_lights\n\nconfig = load_config\\(\\)\nconn = sqlite3.connect\\(r''C:\\\\dev\\\\claude-code\\\\eps-momentum-us\\\\eps_momentum_data.db''\\)\ntoday = pd.read_sql\\(''SELECT MAX\\(date\\) as d FROM ntm_screening'', conn\\)[''d''].iloc[0]\ndf = pd.read_sql\\(f\"\"SELECT * FROM ntm_screening WHERE date=''{today}'' AND is_turnaround=0 ORDER BY rank\"\", conn\\)\nconn.close\\(\\)\n\ncache = {}\ncp = os.path.join\\(r''C:\\\\dev\\\\claude-code\\\\eps-momentum-us'', ''ticker_info_cache.json''\\)\nwith open\\(cp, ''r'', encoding=''utf-8''\\) as f:\n    cache = json.load\\(f\\)\n\nfor idx, row in df.iterrows\\(\\):\n    t = row[''ticker'']\n    if t in cache:\n        df.at[idx, ''short_name''] = cache[t].get\\(''shortName'', t\\)\n        df.at[idx, ''industry''] = cache[t].get\\(''industry'', ''''\\)\n    ntm = {''current'': row[''ntm_current''], ''7d'': row[''ntm_7d''], ''30d'': row[''ntm_30d''], ''60d'': row[''ntm_60d''], ''90d'': row[''ntm_90d'']}\n    score_val, s1, s2, s3, s4, _ = calculate_ntm_score\\(ntm\\)\n    lights, desc = get_trend_lights\\(s1, s2, s3, s4\\)\n    df.at[idx, ''trend_lights''] = lights\n    df.at[idx, ''trend_desc''] = desc\n    df.at[idx, ''ntm_cur''] = row[''ntm_current'']\n\n# Part 1\nmsg_part1 = create_part1_message\\(df\\)\n\n# Part 2 with full price data\ncandidates = df[df[''score''] > 3].copy\\(\\)\ntickers_all = candidates[''ticker''].tolist\\(\\)\nprint\\(f''Downloading prices for {len\\(tickers_all\\)} stocks...''\\)\nprice_data = yf.download\\(tickers_all, period=''100d'', progress=False\\)\n\nfor idx, row in candidates.iterrows\\(\\):\n    t = row[''ticker'']\n    nc = row[''ntm_current'']\n    n7, n30, n60, n90 = row[''ntm_7d''], row[''ntm_30d''], row[''ntm_60d''], row[''ntm_90d'']\n    if n90 != 0:\n        df.at[idx, ''eps_change_90d''] = \\(nc - n90\\) / abs\\(n90\\) * 100\n    else:\n        df.at[idx, ''eps_change_90d''] = 0\n    segs = []\n    for newer, older in [\\(nc, n7\\), \\(n7, n30\\), \\(n30, n60\\), \\(n60, n90\\)]:\n        segs.append\\(\\(newer - older\\) / abs\\(older\\) * 100 if older != 0 else 0\\)\n    df.at[idx, ''eps_chg_weighted''] = segs[0]*0.4 + segs[1]*0.3 + segs[2]*0.2 + segs[3]*0.1\n    try:\n        if t in price_data[''Close''].columns:\n            prices = price_data[''Close''][t].dropna\\(\\)\n            if len\\(prices\\) >= 2:\n                cur = prices.iloc[-1]\n                p7 = prices.iloc[-6] if len\\(prices\\) >= 6 else prices.iloc[0]\n                p30 = prices.iloc[-23] if len\\(prices\\) >= 23 else prices.iloc[0]\n                p60 = prices.iloc[-46] if len\\(prices\\) >= 46 else prices.iloc[0]\n                p90 = prices.iloc[-69] if len\\(prices\\) >= 69 else prices.iloc[0]\n                pc = [\\(cur/p7-1\\)*100, \\(cur/p30-1\\)*100, \\(cur/p60-1\\)*100, \\(cur/p90-1\\)*100]\n                df.at[idx, ''price_chg_weighted''] = pc[0]*0.4 + pc[1]*0.3 + pc[2]*0.2 + pc[3]*0.1\n                if nc > 0:\n                    df.at[idx, ''fwd_pe''] = cur / nc\n                    if n90 > 0:\n                        df.at[idx, ''fwd_pe_chg''] = \\(cur/nc - p90/n90\\)\n    except:\n        pass\n\nmsg_part2 = create_part2_message\\(df\\)\nprint\\(f''Part 1: {len\\(msg_part1\\)} chars, Part 2: {len\\(msg_part2\\)} chars''\\)\n\n# AI analysis\nprint\\(''Calling Gemini...''\\)\nmsg_ai = run_ai_analysis\\(msg_part1, msg_part2, None, config\\)\n\nif msg_ai:\n    print\\(f''AI result: {len\\(msg_ai\\)} chars''\\)\n    private_id = config.get\\(''telegram_private_id''\\) or config.get\\(''telegram_chat_id''\\)\n    success = send_telegram_long\\(msg_ai, config, chat_id=private_id\\)\n    print\\(f''Send: {\"\"OK\"\" if success else \"\"FAIL\"\"}''\\)\nelse:\n    print\\(''AI analysis failed''\\)\n\")",
      "Bash(\"C:\\\\Users\\\\jkw88\\\\miniconda3\\\\envs\\\\volumequant\\\\python.exe\" -c \"\nimport sys, os, json, sqlite3\nimport pandas as pd\nimport yfinance as yf\nsys.path.insert\\(0, r''C:\\\\dev\\\\claude-code\\\\eps-momentum-us''\\)\nfrom daily_runner import load_config, create_part1_message, create_part2_message, send_telegram_long\nfrom eps_momentum_system import calculate_ntm_score, get_trend_lights\n\nconfig = load_config\\(\\)\nconn = sqlite3.connect\\(r''C:\\\\dev\\\\claude-code\\\\eps-momentum-us\\\\eps_momentum_data.db''\\)\ntoday = pd.read_sql\\(''SELECT MAX\\(date\\) as d FROM ntm_screening'', conn\\)[''d''].iloc[0]\ndf = pd.read_sql\\(f\"\"SELECT * FROM ntm_screening WHERE date=''{today}'' AND is_turnaround=0 ORDER BY rank\"\", conn\\)\nconn.close\\(\\)\n\ncache = {}\nwith open\\(os.path.join\\(r''C:\\\\dev\\\\claude-code\\\\eps-momentum-us'', ''ticker_info_cache.json''\\), ''r'', encoding=''utf-8''\\) as f:\n    cache = json.load\\(f\\)\n\nfor idx, row in df.iterrows\\(\\):\n    t = row[''ticker'']\n    if t in cache:\n        df.at[idx, ''short_name''] = cache[t].get\\(''shortName'', t\\)\n        df.at[idx, ''industry''] = cache[t].get\\(''industry'', ''''\\)\n    ntm = {''current'': row[''ntm_current''], ''7d'': row[''ntm_7d''], ''30d'': row[''ntm_30d''], ''60d'': row[''ntm_60d''], ''90d'': row[''ntm_90d'']}\n    score_val, s1, s2, s3, s4, _ = calculate_ntm_score\\(ntm\\)\n    lights, desc = get_trend_lights\\(s1, s2, s3, s4\\)\n    df.at[idx, ''trend_lights''] = lights\n    df.at[idx, ''trend_desc''] = desc\n    df.at[idx, ''ntm_cur''] = row[''ntm_current'']\n\ncandidates = df[df[''score''] > 3].copy\\(\\)\ntickers_all = candidates[''ticker''].tolist\\(\\)\nprint\\(f''Downloading prices for {len\\(tickers_all\\)} stocks...''\\)\nprice_data = yf.download\\(tickers_all, period=''100d'', progress=False\\)\n\nfor idx, row in candidates.iterrows\\(\\):\n    t = row[''ticker'']\n    nc = row[''ntm_current'']\n    n7, n30, n60, n90 = row[''ntm_7d''], row[''ntm_30d''], row[''ntm_60d''], row[''ntm_90d'']\n    if n90 != 0:\n        df.at[idx, ''eps_change_90d''] = \\(nc - n90\\) / abs\\(n90\\) * 100\n    else:\n        df.at[idx, ''eps_change_90d''] = 0\n    segs = []\n    for newer, older in [\\(nc, n7\\), \\(n7, n30\\), \\(n30, n60\\), \\(n60, n90\\)]:\n        segs.append\\(\\(newer - older\\) / abs\\(older\\) * 100 if older != 0 else 0\\)\n    df.at[idx, ''eps_chg_weighted''] = segs[0]*0.4 + segs[1]*0.3 + segs[2]*0.2 + segs[3]*0.1\n    try:\n        if t in price_data[''Close''].columns:\n            prices = price_data[''Close''][t].dropna\\(\\)\n            if len\\(prices\\) >= 2:\n                cur = prices.iloc[-1]\n                p7 = prices.iloc[-6] if len\\(prices\\) >= 6 else prices.iloc[0]\n                p30 = prices.iloc[-23] if len\\(prices\\) >= 23 else prices.iloc[0]\n                p60 = prices.iloc[-46] if len\\(prices\\) >= 46 else prices.iloc[0]\n                p90 = prices.iloc[-69] if len\\(prices\\) >= 69 else prices.iloc[0]\n                pc = [\\(cur/p7-1\\)*100, \\(cur/p30-1\\)*100, \\(cur/p60-1\\)*100, \\(cur/p90-1\\)*100]\n                df.at[idx, ''price_chg_weighted''] = pc[0]*0.4 + pc[1]*0.3 + pc[2]*0.2 + pc[3]*0.1\n                if nc > 0:\n                    fwd_pe_now = cur / nc\n                    df.at[idx, ''fwd_pe''] = fwd_pe_now\n                    # 가중평균 괴리율 \\(run_ntm_collection과 동일 로직\\)\n                    weights = {''7d'': \\(n7, p7, 0.4\\), ''30d'': \\(n30, p30, 0.3\\), ''60d'': \\(n60, p60, 0.2\\), ''90d'': \\(n90, p90, 0.1\\)}\n                    w_sum, w_total = 0.0, 0.0\n                    for k, \\(ntm_val, p_val, w\\) in weights.items\\(\\):\n                        if ntm_val > 0 and p_val > 0:\n                            pe_then = p_val / ntm_val\n                            pe_chg = \\(fwd_pe_now - pe_then\\) / pe_then * 100\n                            w_sum += w * pe_chg\n                            w_total += w\n                    if w_total > 0:\n                        df.at[idx, ''fwd_pe_chg''] = w_sum / w_total\n    except:\n        pass\n\nmsg1 = create_part1_message\\(df\\)\nmsg2 = create_part2_message\\(df\\)\nprint\\(f''Part 1: {len\\(msg1\\)} chars, Part 2: {len\\(msg2\\) if msg2 else 0} chars''\\)\n\nprivate_id = config.get\\(''telegram_private_id''\\) or config.get\\(''telegram_chat_id''\\)\nsend_telegram_long\\(msg1, config, chat_id=private_id\\)\nsend_telegram_long\\(msg2, config, chat_id=private_id\\)\nprint\\(''Both sent''\\)\n\")",
      "Bash(C:\\\\Users\\\\jkw88\\\\miniconda3\\\\envs\\\\volumequant\\\\python.exe:*)",
      "Bash(/tmp/test_part2.py << 'PYEOF'\n\"\"\"Part 2 테스트 - DB 데이터 + 가격 데이터로 메시지 생성 및 전송\"\"\"\nimport sys, os\nsys.path.insert\\(0, r'C:\\\\dev\\\\claude-code\\\\eps-momentum-us'\\)\n\nfrom daily_runner import \\(load_config, send_telegram_long, create_part2_message, DB_PATH\\)\nfrom eps_momentum_system import calculate_ntm_score, get_trend_lights, calculate_eps_change_90d\nimport json, sqlite3\nimport pandas as pd\nimport yfinance as yf\n\nconfig = load_config\\(\\)\nconn = sqlite3.connect\\(str\\(DB_PATH\\)\\)\n\n# Load cache\ncache = {}\ncp = os.path.join\\(r'C:\\\\dev\\\\claude-code\\\\eps-momentum-us', 'ticker_info_cache.json'\\)\nif os.path.exists\\(cp\\):\n    with open\\(cp, 'r', encoding='utf-8'\\) as f:\n        cache = json.load\\(f\\)\n\n# Latest date data\ntoday_date = pd.read_sql\\('SELECT MAX\\(date\\) as d FROM ntm_screening', conn\\)['d'].iloc[0]\ndf_main = pd.read_sql\\(f\"SELECT * FROM ntm_screening WHERE date='{today_date}' AND is_turnaround=0 ORDER BY rank\", conn\\)\n\nprint\\(f\"Date: {today_date}, Main: {len\\(df_main\\)}\"\\)\n\n# Enrich with cache + score details\nfor idx, row in df_main.iterrows\\(\\):\n    t = row['ticker']\n    if t in cache:\n        df_main.at[idx, 'short_name'] = cache[t].get\\('shortName', t\\)\n        df_main.at[idx, 'industry'] = cache[t].get\\('industry', ''\\)\n    ntm = {'current': row['ntm_current'], '7d': row['ntm_7d'], '30d': row['ntm_30d'],\n           '60d': row['ntm_60d'], '90d': row['ntm_90d']}\n    score_val, s1, s2, s3, s4, _ = calculate_ntm_score\\(ntm\\)\n    lights, desc = get_trend_lights\\(s1, s2, s3, s4\\)\n    df_main.at[idx, 'trend_lights'] = lights\n    df_main.at[idx, 'trend_desc'] = desc\n    df_main.at[idx, 'eps_change_90d'] = calculate_eps_change_90d\\(ntm\\)\n\n# Score > 10 stocks for price download\nscore10_tickers = df_main[df_main['score'] > 10]['ticker'].tolist\\(\\)\nprint\\(f\"Score > 10: {len\\(score10_tickers\\)} stocks\"\\)\n\n# Download prices\nprint\\(\"Downloading prices...\"\\)\nprice_data = yf.download\\(score10_tickers, period='100d', progress=False\\)\n\nfor idx, row in df_main.iterrows\\(\\):\n    t = row['ticker']\n    nc = row['ntm_current']\n    n7, n30, n60, n90 = row['ntm_7d'], row['ntm_30d'], row['ntm_60d'], row['ntm_90d']\n\n    # Weighted EPS change\n    segs = []\n    for newer, older in [\\(nc, n7\\), \\(n7, n30\\), \\(n30, n60\\), \\(n60, n90\\)]:\n        if older != 0:\n            segs.append\\(\\(newer - older\\) / abs\\(older\\) * 100\\)\n        else:\n            segs.append\\(0\\)\n    eps_w = segs[0]*0.4 + segs[1]*0.3 + segs[2]*0.2 + segs[3]*0.1\n    df_main.at[idx, 'eps_chg_weighted'] = eps_w\n\n    try:\n        if t not in score10_tickers:\n            continue\n        col = price_data['Close'][t] if t in price_data['Close'].columns else None\n        if col is None:\n            continue\n        prices_series = col.dropna\\(\\)\n        if len\\(prices_series\\) < 2:\n            continue\n\n        p_now = float\\(prices_series.iloc[-1]\\)\n        p7 = float\\(prices_series.iloc[-6]\\) if len\\(prices_series\\) >= 6 else float\\(prices_series.iloc[0]\\)\n        p30 = float\\(prices_series.iloc[-23]\\) if len\\(prices_series\\) >= 23 else float\\(prices_series.iloc[0]\\)\n        p60 = float\\(prices_series.iloc[-46]\\) if len\\(prices_series\\) >= 46 else float\\(prices_series.iloc[0]\\)\n        p90 = float\\(prices_series.iloc[-69]\\) if len\\(prices_series\\) >= 69 else float\\(prices_series.iloc[0]\\)\n\n        prices = {'7d': p7, '30d': p30, '60d': p60, '90d': p90}\n\n        # 90d price change\n        if p90 > 0:\n            df_main.at[idx, 'price_chg'] = \\(p_now - p90\\) / p90 * 100\n\n        # Weighted price change\n        price_w = {'7d': 0.4, '30d': 0.3, '60d': 0.2, '90d': 0.1}\n        pw_sum = sum\\(w * \\(p_now - prices[k]\\) / prices[k] * 100 for k, w in price_w.items\\(\\) if prices[k] > 0\\)\n        pw_total = sum\\(w for k, w in price_w.items\\(\\) if prices[k] > 0\\)\n        df_main.at[idx, 'price_chg_weighted'] = pw_sum / pw_total if pw_total > 0 else None\n\n        # Fwd PE\n        if nc > 0:\n            fwd_pe_now = p_now / nc\n            df_main.at[idx, 'fwd_pe'] = fwd_pe_now\n\n            # Weighted fwd_pe_chg\n            weights = {'7d': 0.4, '30d': 0.3, '60d': 0.2, '90d': 0.1}\n            weighted_sum = 0.0\n            total_weight = 0.0\n            ntm_vals = {'7d': n7, '30d': n30, '60d': n60, '90d': n90}\n            for key, w in weights.items\\(\\):\n                ntm_val = ntm_vals[key]\n                if nc > 0 and ntm_val > 0 and prices[key] > 0:\n                    fwd_pe_then = prices[key] / ntm_val\n                    pe_chg_period = \\(fwd_pe_now - fwd_pe_then\\) / fwd_pe_then * 100\n                    weighted_sum += w * pe_chg_period\n                    total_weight += w\n            if total_weight > 0:\n                df_main.at[idx, 'fwd_pe_chg'] = weighted_sum / total_weight\n    except Exception as e:\n        pass\n\n# Generate message\nmsg_part2 = create_part2_message\\(df_main\\)\nif msg_part2:\n    print\\(f\"\\\\nPart 2 message: {len\\(msg_part2\\)} chars\"\\)\n    private_id = config.get\\('telegram_private_id'\\) or config.get\\('telegram_chat_id'\\)\n    success = send_telegram_long\\(msg_part2, config, chat_id=private_id\\)\n    print\\(f\"Send: {'OK' if success else 'FAIL'}\"\\)\nelse:\n    print\\(\"No Part 2 message generated\"\\)\n\nconn.close\\(\\)\nPYEOF)",
      "Bash(C:Usersjkw88miniconda3envsvolumequantpython.exe /tmp/test_part2.py)",
      "Bash(\"C:\\\\Users\\\\jkw88\\\\AppData\\\\Local\\\\Temp\\\\claude\\\\C--dev-claude-code-eps-momentum-us\\\\11255f29-175e-42e8-ad10-177043141c2a\\\\scratchpad\\\\test_all_messages.py\" << 'PYEOF'\n\"\"\"Part 1 + Part 2 + AI 리스크 체크 테스트 - DB 실데이터 활용\"\"\"\nimport sys, os\nsys.path.insert\\(0, r'C:\\\\dev\\\\claude-code\\\\eps-momentum-us'\\)\n\nfrom daily_runner import \\(load_config, send_telegram_long, create_part1_message,\n                          create_part2_message, run_ai_analysis, DB_PATH\\)\nfrom eps_momentum_system import calculate_ntm_score, get_trend_lights, calculate_eps_change_90d\nimport json, sqlite3\nimport pandas as pd\nimport yfinance as yf\n\nconfig = load_config\\(\\)\nconn = sqlite3.connect\\(str\\(DB_PATH\\)\\)\n\n# Load cache\ncache = {}\ncp = os.path.join\\(r'C:\\\\dev\\\\claude-code\\\\eps-momentum-us', 'ticker_info_cache.json'\\)\nif os.path.exists\\(cp\\):\n    with open\\(cp, 'r', encoding='utf-8'\\) as f:\n        cache = json.load\\(f\\)\n\n# Latest date data \\(non-turnaround only\\)\ntoday_date = pd.read_sql\\('SELECT MAX\\(date\\) as d FROM ntm_screening', conn\\)['d'].iloc[0]\ndf_main = pd.read_sql\\(f\"SELECT * FROM ntm_screening WHERE date='{today_date}' AND is_turnaround=0 ORDER BY rank\", conn\\)\nprint\\(f\"Date: {today_date}, Main: {len\\(df_main\\)}\"\\)\n\n# Enrich with cache + score details\nfor idx, row in df_main.iterrows\\(\\):\n    t = row['ticker']\n    if t in cache:\n        df_main.at[idx, 'short_name'] = cache[t].get\\('shortName', t\\)\n        df_main.at[idx, 'industry'] = cache[t].get\\('industry', ''\\)\n    ntm = {'current': row['ntm_current'], '7d': row['ntm_7d'], '30d': row['ntm_30d'],\n           '60d': row['ntm_60d'], '90d': row['ntm_90d']}\n    score_val, s1, s2, s3, s4, _ = calculate_ntm_score\\(ntm\\)\n    lights, desc = get_trend_lights\\(s1, s2, s3, s4\\)\n    df_main.at[idx, 'trend_lights'] = lights\n    df_main.at[idx, 'trend_desc'] = desc\n    df_main.at[idx, 'eps_change_90d'] = calculate_eps_change_90d\\(ntm\\)\n\n# === Part 1 ===\nprint\\(\"\\\\n=== Part 1 생성 중... ===\"\\)\nmsg_part1 = create_part1_message\\(df_main\\)\nprint\\(f\"Part 1: {len\\(msg_part1\\)} chars\"\\)\n\n# === Price data for Part 2 \\(Score > 10 stocks\\) ===\nscore10_tickers = df_main[df_main['score'] > 10]['ticker'].tolist\\(\\)\nprint\\(f\"\\\\nScore > 10: {len\\(score10_tickers\\)} stocks\"\\)\nprint\\(\"Downloading prices...\"\\)\nprice_data = yf.download\\(score10_tickers, period='100d', progress=False\\)\n\nfor idx, row in df_main.iterrows\\(\\):\n    t = row['ticker']\n    nc = row['ntm_current']\n    n7, n30, n60, n90 = row['ntm_7d'], row['ntm_30d'], row['ntm_60d'], row['ntm_90d']\n\n    # Weighted EPS change\n    segs = []\n    for newer, older in [\\(nc, n7\\), \\(n7, n30\\), \\(n30, n60\\), \\(n60, n90\\)]:\n        if older != 0:\n            segs.append\\(\\(newer - older\\) / abs\\(older\\) * 100\\)\n        else:\n            segs.append\\(0\\)\n    eps_w = segs[0]*0.4 + segs[1]*0.3 + segs[2]*0.2 + segs[3]*0.1\n    df_main.at[idx, 'eps_chg_weighted'] = eps_w\n\n    try:\n        if t not in score10_tickers:\n            continue\n        col = price_data['Close'][t] if t in price_data['Close'].columns else None\n        if col is None:\n            continue\n        prices_series = col.dropna\\(\\)\n        if len\\(prices_series\\) < 2:\n            continue\n\n        p_now = float\\(prices_series.iloc[-1]\\)\n        p7 = float\\(prices_series.iloc[-6]\\) if len\\(prices_series\\) >= 6 else float\\(prices_series.iloc[0]\\)\n        p30 = float\\(prices_series.iloc[-23]\\) if len\\(prices_series\\) >= 23 else float\\(prices_series.iloc[0]\\)\n        p60 = float\\(prices_series.iloc[-46]\\) if len\\(prices_series\\) >= 46 else float\\(prices_series.iloc[0]\\)\n        p90 = float\\(prices_series.iloc[-69]\\) if len\\(prices_series\\) >= 69 else float\\(prices_series.iloc[0]\\)\n\n        prices = {'7d': p7, '30d': p30, '60d': p60, '90d': p90}\n\n        # 90d price change\n        if p90 > 0:\n            df_main.at[idx, 'price_chg'] = \\(p_now - p90\\) / p90 * 100\n\n        # Weighted price change\n        price_w = {'7d': 0.4, '30d': 0.3, '60d': 0.2, '90d': 0.1}\n        pw_sum = sum\\(w * \\(p_now - prices[k]\\) / prices[k] * 100 for k, w in price_w.items\\(\\) if prices[k] > 0\\)\n        pw_total = sum\\(w for k, w in price_w.items\\(\\) if prices[k] > 0\\)\n        df_main.at[idx, 'price_chg_weighted'] = pw_sum / pw_total if pw_total > 0 else None\n\n        # Fwd PE\n        if nc > 0:\n            fwd_pe_now = p_now / nc\n            df_main.at[idx, 'fwd_pe'] = fwd_pe_now\n\n            # Weighted fwd_pe_chg\n            weights = {'7d': 0.4, '30d': 0.3, '60d': 0.2, '90d': 0.1}\n            weighted_sum = 0.0\n            total_weight = 0.0\n            ntm_vals = {'7d': n7, '30d': n30, '60d': n60, '90d': n90}\n            for key, w in weights.items\\(\\):\n                ntm_val = ntm_vals[key]\n                if nc > 0 and ntm_val > 0 and prices[key] > 0:\n                    fwd_pe_then = prices[key] / ntm_val\n                    pe_chg_period = \\(fwd_pe_now - fwd_pe_then\\) / fwd_pe_then * 100\n                    weighted_sum += w * pe_chg_period\n                    total_weight += w\n            if total_weight > 0:\n                df_main.at[idx, 'fwd_pe_chg'] = weighted_sum / total_weight\n    except Exception as e:\n        pass\n\n# === Part 2 ===\nprint\\(\"\\\\n=== Part 2 생성 중... ===\"\\)\nmsg_part2 = create_part2_message\\(df_main\\)\nprint\\(f\"Part 2: {len\\(msg_part2\\) if msg_part2 else 0} chars\"\\)\n\n# === Send Part 1 & Part 2 ===\nprivate_id = config.get\\('telegram_private_id'\\) or config.get\\('telegram_chat_id'\\)\n\nprint\\(\"\\\\n=== 텔레그램 전송 ===\"\\)\nok1 = send_telegram_long\\(msg_part1, config, chat_id=private_id\\)\nprint\\(f\"Part 1 send: {'OK' if ok1 else 'FAIL'}\"\\)\n\nif msg_part2:\n    ok2 = send_telegram_long\\(msg_part2, config, chat_id=private_id\\)\n    print\\(f\"Part 2 send: {'OK' if ok2 else 'FAIL'}\"\\)\n\n# === AI Analysis ===\nprint\\(\"\\\\n=== AI 리스크 체크 생성 중... ===\"\\)\nmsg_ai = run_ai_analysis\\(msg_part1, msg_part2, None, config\\)\nif msg_ai:\n    print\\(f\"AI: {len\\(msg_ai\\)} chars\"\\)\n    ok3 = send_telegram_long\\(msg_ai, config, chat_id=private_id\\)\n    print\\(f\"AI send: {'OK' if ok3 else 'FAIL'}\"\\)\nelse:\n    print\\(\"AI analysis failed\"\\)\n\nconn.close\\(\\)\nprint\\(\"\\\\n=== 완료 ===\"\\)\nPYEOF)",
      "Bash(C:Usersjkw88miniconda3envsvolumequantpython.exe:*)",
      "Bash(cmd.exe /c \"C:\\\\Users\\\\jkw88\\\\miniconda3\\\\envs\\\\volumequant\\\\python.exe -c \"\"print\\(''hello''\\)\"\"\")",
      "Bash(echo:*)",
      "Bash(/c/dev/claude-code/eps-momentum-us/_tmp_analysis.py:*)",
      "Bash(PYTHONIOENCODING=utf-8 /c/Users/jkw88/miniconda3/envs/volumequant/python.exe:*)",
      "Bash(powershell.exe -Command \"Get-Command python\")",
      "Bash(\"C:\\\\dev\\\\claude-code\\\\eps-momentum-us\\\\_accel_analysis.py\" << 'PYEOF'\nimport sys\nsys.path.insert\\(0, r'C:\\\\dev\\\\claude-code\\\\eps-momentum-us'\\)\nimport sqlite3, pandas as pd, json, os\nfrom eps_momentum_system import calculate_ntm_score, get_trend_lights\n\nDB_PATH = r'C:\\\\dev\\\\claude-code\\\\eps-momentum-us\\\\eps_momentum_data.db'\nconn = sqlite3.connect\\(DB_PATH\\)\ntoday_date = pd.read_sql\\('SELECT MAX\\(date\\) as d FROM ntm_screening', conn\\)['d'].iloc[0]\ndf = pd.read_sql\\(f\"SELECT * FROM ntm_screening WHERE date='{today_date}' AND is_turnaround=0\", conn\\)\n\n# Load cache for names\ncache = {}\ncp = os.path.join\\(r'C:\\\\dev\\\\claude-code\\\\eps-momentum-us', 'ticker_info_cache.json'\\)\nif os.path.exists\\(cp\\):\n    with open\\(cp, 'r', encoding='utf-8'\\) as f:\n        cache = json.load\\(f\\)\n\nrows = []\nfor idx, row in df.iterrows\\(\\):\n    ntm = {'current': row['ntm_current'], '7d': row['ntm_7d'], '30d': row['ntm_30d'],\n           '60d': row['ntm_60d'], '90d': row['ntm_90d']}\n    score, s1, s2, s3, s4, _ = calculate_ntm_score\\(ntm\\)\n    lights, desc = get_trend_lights\\(s1, s2, s3, s4\\)\n    \n    # Acceleration: recent half vs old half\n    recent = \\(s1 + s2\\) / 2  # last ~30 days\n    old = \\(s3 + s4\\) / 2     # 30~90 days ago\n    accel = recent - old     # positive = accelerating\n    \n    pos_count = sum\\(1 for s in [s1, s2, s3, s4] if s > 0\\)\n    name = cache.get\\(row['ticker'], {}\\).get\\('shortName', row['ticker']\\)\n    \n    rows.append\\({\n        'ticker': row['ticker'], 'name': name, 'score': score,\n        's1': s1, 's2': s2, 's3': s3, 's4': s4,\n        'lights': lights, 'desc': desc,\n        'recent': recent, 'old': old, 'accel': accel,\n        'pos_count': pos_count\n    }\\)\n\ndf2 = pd.DataFrame\\(rows\\)\ndf2 = df2.sort_values\\('score', ascending=False\\).reset_index\\(drop=True\\)\ndf2['orig_rank'] = range\\(1, len\\(df2\\)+1\\)\n\n# === 1. Score > 0 distribution ===\npos_df = df2[df2['score'] > 0]\nprint\\('=== Score > 0 전체 \\({} 종목\\) ===\\\\n'.format\\(len\\(pos_df\\)\\)\\)\naccelerating = pos_df[pos_df['accel'] > 5]\ndecelerating = pos_df[pos_df['accel'] < -5]\nsteady = pos_df[\\(pos_df['accel'] >= -5\\) & \\(pos_df['accel'] <= 5\\)]\nprint\\(f'가속 \\(최근 >> 과거, accel > 5\\):  {len\\(accelerating\\)}종목 \\({len\\(accelerating\\)*100//len\\(pos_df\\)}%\\)'\\)\nprint\\(f'안정 \\(비슷, |accel| <= 5\\):       {len\\(steady\\)}종목 \\({len\\(steady\\)*100//len\\(pos_df\\)}%\\)'\\)\nprint\\(f'감속 \\(과거 >> 최근, accel < -5\\):  {len\\(decelerating\\)}종목 \\({len\\(decelerating\\)*100//len\\(pos_df\\)}%\\)'\\)\n\n# === 2. Score 10~30 band ===\nprint\\('\\\\n=== Score 10~30 구간: 가속 vs 감속 대표 종목 ===\\\\n'\\)\nmid_df = df2[\\(df2['score'] >= 10\\) & \\(df2['score'] <= 30\\)]\n\nprint\\('>>> 가속 중 \\(과거 약->최근 강\\):'\\)\naccel_mid = mid_df[mid_df['accel'] > 5].sort_values\\('accel', ascending=False\\).head\\(10\\)\nfor _, r in accel_mid.iterrows\\(\\):\n    print\\(f\"  {r['orig_rank']:3d}위 {r['ticker']:6s} Score={r['score']:5.1f} | {r['lights']} {r['desc']:6s} | 과거\\({r['old']:+5.1f}\\) -> 최근\\({r['recent']:+5.1f}\\) 가속={r['accel']:+.1f}\"\\)\n\nprint\\('\\\\n>>> 감속 중 \\(과거 강->최근 약\\):'\\)\ndecel_mid = mid_df[mid_df['accel'] < -5].sort_values\\('accel'\\).head\\(10\\)\nfor _, r in decel_mid.iterrows\\(\\):\n    print\\(f\"  {r['orig_rank']:3d}위 {r['ticker']:6s} Score={r['score']:5.1f} | {r['lights']} {r['desc']:6s} | 과거\\({r['old']:+5.1f}\\) -> 최근\\({r['recent']:+5.1f}\\) 가속={r['accel']:+.1f}\"\\)\n\n# === 3. Same score, opposite direction ===\nprint\\('\\\\n=== 비슷한 Score, 반대 방향 -- 핵심 비교 ===\\\\n'\\)\nfor score_center in [12, 15, 20, 25]:\n    band = df2[\\(df2['score'] >= score_center - 2\\) & \\(df2['score'] <= score_center + 2\\)]\n    acc = band[band['accel'] > 3].sort_values\\('accel', ascending=False\\).head\\(1\\)\n    dec = band[band['accel'] < -3].sort_values\\('accel'\\).head\\(1\\)\n    if len\\(acc\\) > 0 and len\\(dec\\) > 0:\n        a = acc.iloc[0]\n        d = dec.iloc[0]\n        print\\(f'Score ~{score_center}:'\\)\n        print\\(f\"  가속: {a['orig_rank']:3d}위 {a['ticker']:6s} Score={a['score']:5.1f} | {a['lights']} | seg: {a['s4']:+5.1f} {a['s3']:+5.1f} {a['s2']:+5.1f} {a['s1']:+5.1f} | accel={a['accel']:+.1f}\"\\)\n        print\\(f\"  감속: {d['orig_rank']:3d}위 {d['ticker']:6s} Score={d['score']:5.1f} | {d['lights']} | seg: {d['s4']:+5.1f} {d['s3']:+5.1f} {d['s2']:+5.1f} {d['s1']:+5.1f} | accel={d['accel']:+.1f}\"\\)\n        print\\(\\)\n\n# === 4. Acceleration adjustment simulation ===\nprint\\('=== 가속 보정 시뮬레이션 ===\\\\n'\\)\nimport numpy as np\ndf2['accel_factor'] = df2['accel'].clip\\(-30, 30\\) / 100  # max +/-30%\ndf2['adj_score'] = df2['score'] * \\(1 + df2['accel_factor']\\)\ndf2_new = df2.sort_values\\('adj_score', ascending=False\\).reset_index\\(drop=True\\)\ndf2_new['new_rank'] = range\\(1, len\\(df2_new\\)+1\\)\n\nmerged = df2_new[['ticker', 'new_rank', 'orig_rank', 'score', 'adj_score', 'lights', 'accel', 'pos_count', 'name']].copy\\(\\)\nmerged['change'] = merged['orig_rank'] - merged['new_rank']\n\nprint\\('보정 후 Top 30:'\\)\nfor _, r in merged.head\\(30\\).iterrows\\(\\):\n    ch = r['change']\n    arrow = f\"+{int\\(ch\\)}\" if ch > 0 else str\\(int\\(ch\\)\\) if ch < 0 else \"=\"\n    print\\(f\"{r['new_rank']:2d}위\\({arrow:>4s}\\) {r['ticker']:6s} {r['score']:5.1f}->{r['adj_score']:5.1f} | {r['lights']} | accel={r['accel']:+5.1f}\"\\)\n\nprint\\('\\\\n큰 순위 변동 \\(|변동| >= 10, Score > 5\\):'\\)\nbig_changes = merged[\\(abs\\(merged['change']\\) >= 10\\) & \\(merged['score'] > 5\\)].sort_values\\('change', ascending=False\\)\nprint\\(f'총 {len\\(big_changes\\)}종목'\\)\nfor _, r in big_changes.head\\(10\\).iterrows\\(\\):\n    print\\(f\"  {r['ticker']:6s} {int\\(r['orig_rank']\\):3d}->{int\\(r['new_rank']\\):3d}위 \\({int\\(r['change']\\):+d}\\) Score={r['score']:5.1f}->{r['adj_score']:5.1f} | {r['lights']} accel={r['accel']:+.1f}\"\\)\nprint\\('...'\\)\nfor _, r in big_changes.tail\\(10\\).iterrows\\(\\):\n    print\\(f\"  {r['ticker']:6s} {int\\(r['orig_rank']\\):3d}->{int\\(r['new_rank']\\):3d}위 \\({int\\(r['change']\\):+d}\\) Score={r['score']:5.1f}->{r['adj_score']:5.1f} | {r['lights']} accel={r['accel']:+.1f}\"\\)\n\n# === 5. Part 2 impact ===\nprint\\('\\\\n=== Part 2 영향 \\(Score > 10 vs adj_score > 10\\) ==='\\)\norig_p2 = set\\(df2[df2['score'] > 10]['ticker']\\)\nnew_p2 = set\\(df2[df2['adj_score'] > 10]['ticker']\\)\nentered = new_p2 - orig_p2\nexited = orig_p2 - new_p2\nprint\\(f'기존 Score > 10: {len\\(orig_p2\\)}종목'\\)\nprint\\(f'보정 adj_score > 10: {len\\(new_p2\\)}종목'\\)\nprint\\(f'신규 진입: {len\\(entered\\)}종목 {list\\(entered\\)[:10]}'\\)\nprint\\(f'퇴출: {len\\(exited\\)}종목 {list\\(exited\\)[:10]}'\\)\n\nconn.close\\(\\)\nPYEOF)",
      "Bash(\"C:\\\\dev\\\\claude-code\\\\eps-momentum-us\\\\_accel_analysis.py\" << 'PYEOF'\nimport sys, io\nsys.stdout = io.TextIOWrapper\\(sys.stdout.buffer, encoding='utf-8', errors='replace'\\)\nsys.path.insert\\(0, r'C:\\\\dev\\\\claude-code\\\\eps-momentum-us'\\)\nimport sqlite3, pandas as pd, json, os\nfrom eps_momentum_system import calculate_ntm_score, get_trend_lights\n\nDB_PATH = r'C:\\\\dev\\\\claude-code\\\\eps-momentum-us\\\\eps_momentum_data.db'\nconn = sqlite3.connect\\(DB_PATH\\)\ntoday_date = pd.read_sql\\('SELECT MAX\\(date\\) as d FROM ntm_screening', conn\\)['d'].iloc[0]\ndf = pd.read_sql\\(f\"SELECT * FROM ntm_screening WHERE date='{today_date}' AND is_turnaround=0\", conn\\)\n\n# Load cache for names\ncache = {}\ncp = os.path.join\\(r'C:\\\\dev\\\\claude-code\\\\eps-momentum-us', 'ticker_info_cache.json'\\)\nif os.path.exists\\(cp\\):\n    with open\\(cp, 'r', encoding='utf-8'\\) as f:\n        cache = json.load\\(f\\)\n\nrows = []\nfor idx, row in df.iterrows\\(\\):\n    ntm = {'current': row['ntm_current'], '7d': row['ntm_7d'], '30d': row['ntm_30d'],\n           '60d': row['ntm_60d'], '90d': row['ntm_90d']}\n    score, s1, s2, s3, s4, _ = calculate_ntm_score\\(ntm\\)\n    lights, desc = get_trend_lights\\(s1, s2, s3, s4\\)\n    \n    # Acceleration: recent half vs old half\n    recent = \\(s1 + s2\\) / 2  # last ~30 days\n    old = \\(s3 + s4\\) / 2     # 30~90 days ago\n    accel = recent - old     # positive = accelerating\n    \n    pos_count = sum\\(1 for s in [s1, s2, s3, s4] if s > 0\\)\n    name = cache.get\\(row['ticker'], {}\\).get\\('shortName', row['ticker']\\)\n    \n    rows.append\\({\n        'ticker': row['ticker'], 'name': name, 'score': score,\n        's1': s1, 's2': s2, 's3': s3, 's4': s4,\n        'lights': lights, 'desc': desc,\n        'recent': recent, 'old': old, 'accel': accel,\n        'pos_count': pos_count\n    }\\)\n\ndf2 = pd.DataFrame\\(rows\\)\ndf2 = df2.sort_values\\('score', ascending=False\\).reset_index\\(drop=True\\)\ndf2['orig_rank'] = range\\(1, len\\(df2\\)+1\\)\n\n# === 1. Score > 0 distribution ===\npos_df = df2[df2['score'] > 0]\nprint\\('=== Score > 0 전체 \\({} 종목\\) ===\\\\n'.format\\(len\\(pos_df\\)\\)\\)\naccelerating = pos_df[pos_df['accel'] > 5]\ndecelerating = pos_df[pos_df['accel'] < -5]\nsteady = pos_df[\\(pos_df['accel'] >= -5\\) & \\(pos_df['accel'] <= 5\\)]\nprint\\(f'가속 \\(최근 >> 과거, accel > 5\\):  {len\\(accelerating\\)}종목 \\({len\\(accelerating\\)*100//len\\(pos_df\\)}%\\)'\\)\nprint\\(f'안정 \\(비슷, |accel| <= 5\\):       {len\\(steady\\)}종목 \\({len\\(steady\\)*100//len\\(pos_df\\)}%\\)'\\)\nprint\\(f'감속 \\(과거 >> 최근, accel < -5\\):  {len\\(decelerating\\)}종목 \\({len\\(decelerating\\)*100//len\\(pos_df\\)}%\\)'\\)\n\n# === 2. Score 10~30 band ===\nprint\\('\\\\n=== Score 10~30 구간: 가속 vs 감속 대표 종목 ===\\\\n'\\)\nmid_df = df2[\\(df2['score'] >= 10\\) & \\(df2['score'] <= 30\\)]\n\nprint\\('>>> 가속 중 \\(과거 약->최근 강\\):'\\)\naccel_mid = mid_df[mid_df['accel'] > 5].sort_values\\('accel', ascending=False\\).head\\(10\\)\nfor _, r in accel_mid.iterrows\\(\\):\n    print\\(f\"  {r['orig_rank']:3d}위 {r['ticker']:6s} Score={r['score']:5.1f} | {r['lights']} {r['desc']:6s} | 과거\\({r['old']:+5.1f}\\) -> 최근\\({r['recent']:+5.1f}\\) 가속={r['accel']:+.1f}\"\\)\n\nprint\\('\\\\n>>> 감속 중 \\(과거 강->최근 약\\):'\\)\ndecel_mid = mid_df[mid_df['accel'] < -5].sort_values\\('accel'\\).head\\(10\\)\nfor _, r in decel_mid.iterrows\\(\\):\n    print\\(f\"  {r['orig_rank']:3d}위 {r['ticker']:6s} Score={r['score']:5.1f} | {r['lights']} {r['desc']:6s} | 과거\\({r['old']:+5.1f}\\) -> 최근\\({r['recent']:+5.1f}\\) 가속={r['accel']:+.1f}\"\\)\n\n# === 3. Same score, opposite direction ===\nprint\\('\\\\n=== 비슷한 Score, 반대 방향 -- 핵심 비교 ===\\\\n'\\)\nfor score_center in [12, 15, 20, 25]:\n    band = df2[\\(df2['score'] >= score_center - 2\\) & \\(df2['score'] <= score_center + 2\\)]\n    acc = band[band['accel'] > 3].sort_values\\('accel', ascending=False\\).head\\(1\\)\n    dec = band[band['accel'] < -3].sort_values\\('accel'\\).head\\(1\\)\n    if len\\(acc\\) > 0 and len\\(dec\\) > 0:\n        a = acc.iloc[0]\n        d = dec.iloc[0]\n        print\\(f'Score ~{score_center}:'\\)\n        print\\(f\"  가속: {a['orig_rank']:3d}위 {a['ticker']:6s} Score={a['score']:5.1f} | {a['lights']} | seg: {a['s4']:+5.1f} {a['s3']:+5.1f} {a['s2']:+5.1f} {a['s1']:+5.1f} | accel={a['accel']:+.1f}\"\\)\n        print\\(f\"  감속: {d['orig_rank']:3d}위 {d['ticker']:6s} Score={d['score']:5.1f} | {d['lights']} | seg: {d['s4']:+5.1f} {d['s3']:+5.1f} {d['s2']:+5.1f} {d['s1']:+5.1f} | accel={d['accel']:+.1f}\"\\)\n        print\\(\\)\n\n# === 4. Acceleration adjustment simulation ===\nprint\\('=== 가속 보정 시뮬레이션 ===\\\\n'\\)\nimport numpy as np\ndf2['accel_factor'] = df2['accel'].clip\\(-30, 30\\) / 100  # max +/-30%\ndf2['adj_score'] = df2['score'] * \\(1 + df2['accel_factor']\\)\ndf2_new = df2.sort_values\\('adj_score', ascending=False\\).reset_index\\(drop=True\\)\ndf2_new['new_rank'] = range\\(1, len\\(df2_new\\)+1\\)\n\nmerged = df2_new[['ticker', 'new_rank', 'orig_rank', 'score', 'adj_score', 'lights', 'accel', 'pos_count', 'name']].copy\\(\\)\nmerged['change'] = merged['orig_rank'] - merged['new_rank']\n\nprint\\('보정 후 Top 30:'\\)\nfor _, r in merged.head\\(30\\).iterrows\\(\\):\n    ch = r['change']\n    arrow = f\"+{int\\(ch\\)}\" if ch > 0 else str\\(int\\(ch\\)\\) if ch < 0 else \"=\"\n    print\\(f\"{r['new_rank']:2d}위\\({arrow:>4s}\\) {r['ticker']:6s} {r['score']:5.1f}->{r['adj_score']:5.1f} | {r['lights']} | accel={r['accel']:+5.1f}\"\\)\n\nprint\\('\\\\n큰 순위 변동 \\(|변동| >= 10, Score > 5\\):'\\)\nbig_changes = merged[\\(abs\\(merged['change']\\) >= 10\\) & \\(merged['score'] > 5\\)].sort_values\\('change', ascending=False\\)\nprint\\(f'총 {len\\(big_changes\\)}종목'\\)\nfor _, r in big_changes.head\\(10\\).iterrows\\(\\):\n    print\\(f\"  {r['ticker']:6s} {int\\(r['orig_rank']\\):3d}->{int\\(r['new_rank']\\):3d}위 \\({int\\(r['change']\\):+d}\\) Score={r['score']:5.1f}->{r['adj_score']:5.1f} | {r['lights']} accel={r['accel']:+.1f}\"\\)\nprint\\('...'\\)\nfor _, r in big_changes.tail\\(10\\).iterrows\\(\\):\n    print\\(f\"  {r['ticker']:6s} {int\\(r['orig_rank']\\):3d}->{int\\(r['new_rank']\\):3d}위 \\({int\\(r['change']\\):+d}\\) Score={r['score']:5.1f}->{r['adj_score']:5.1f} | {r['lights']} accel={r['accel']:+.1f}\"\\)\n\n# === 5. Part 2 impact ===\nprint\\('\\\\n=== Part 2 영향 \\(Score > 10 vs adj_score > 10\\) ==='\\)\norig_p2 = set\\(df2[df2['score'] > 10]['ticker']\\)\nnew_p2 = set\\(df2[df2['adj_score'] > 10]['ticker']\\)\nentered = new_p2 - orig_p2\nexited = orig_p2 - new_p2\nprint\\(f'기존 Score > 10: {len\\(orig_p2\\)}종목'\\)\nprint\\(f'보정 adj_score > 10: {len\\(new_p2\\)}종목'\\)\nprint\\(f'신규 진입: {len\\(entered\\)}종목 {list\\(entered\\)[:10]}'\\)\nprint\\(f'퇴출: {len\\(exited\\)}종목 {list\\(exited\\)[:10]}'\\)\n\nconn.close\\(\\)\nPYEOF)",
      "Bash(cmd /c:*)",
      "Bash(powershell.exe -Command \"& ''C:\\\\Users\\\\jkw88\\\\miniconda3\\\\envs\\\\volumequant\\\\python.exe'' ''C:\\\\dev\\\\claude-code\\\\eps-momentum-us\\\\pattern_analysis.py'' 2>&1 | Out-File -FilePath ''C:\\\\dev\\\\claude-code\\\\eps-momentum-us\\\\pattern_output.txt'' -Encoding utf8\")",
      "Bash(cmd /c \"C:\\\\Users\\\\jkw88\\\\miniconda3\\\\envs\\\\volumequant\\\\python.exe -c \"\"import yfinance as yf; import pandas as pd; test_tickers = [''MSFT'', ''NVDA'', ''AAPL'', ''META'', ''ALB'']; [print\\(f''\\\\n{t}\\\\n'', yf.Ticker\\(t\\).revenue_estimate\\) for t in test_tickers]\"\"\")",
      "Bash(powershell.exe -Command \"& ''C:\\\\Users\\\\jkw88\\\\miniconda3\\\\envs\\\\volumequant\\\\python.exe'' -c ''print\\(42\\)''\")",
      "Bash(powershell.exe:*)",
      "Bash(cmd.exe:*)",
      "Bash(for f in test_syntax.py test_direction_scoring.py pattern_analysis.py pattern_output.txt threshold_analysis.py)",
      "Bash(do git log --oneline --all -- \"$f\")",
      "Bash(done)",
      "Bash(powershell.exe -Command \"& ''C:\\\\Users\\\\jkw88\\\\miniconda3\\\\envs\\\\volumequant\\\\python.exe'' -c ''print\\(\"\"hello world\"\"\\)''\")",
      "Bash(powershell.exe -Command \"& ''C:\\\\Users\\\\jkw88\\\\miniconda3\\\\envs\\\\volumequant\\\\python.exe'' -c ''print\\(123\\)''\")",
      "Bash(powershell.exe -Command \"Set-Location ''C:\\\\dev\\\\claude-code\\\\eps-momentum-us''; & ''C:\\\\Users\\\\jkw88\\\\miniconda3\\\\envs\\\\volumequant\\\\python.exe'' test_ai_prompt.py 2>&1\")",
      "Bash(cmd.exe /c \"cd /d C:\\\\dev\\\\claude-code\\\\eps-momentum-us && C:\\\\Users\\\\jkw88\\\\miniconda3\\\\envs\\\\volumequant\\\\python.exe test_patterns.py\")",
      "Bash(cmd.exe /c \"chcp 65001 >nul 2>&1 && cd /d C:\\\\dev\\\\claude-code\\\\eps-momentum-us && set PYTHONIOENCODING=utf-8 && C:\\\\Users\\\\jkw88\\\\miniconda3\\\\envs\\\\volumequant\\\\python.exe test_patterns.py 2>&1\")",
      "Bash(cmd.exe /c \"cd /d C:\\\\dev\\\\claude-code\\\\eps-momentum-us && set PYTHONIOENCODING=utf-8 && C:\\\\Users\\\\jkw88\\\\miniconda3\\\\envs\\\\volumequant\\\\python.exe test_patterns.py > test_output.txt 2>&1\")",
      "Bash(powershell.exe -Command \"Remove-Item ''C:\\\\dev\\\\claude-code\\\\eps-momentum-us\\\\test_ai_prompt.py''; Remove-Item ''C:\\\\dev\\\\claude-code\\\\eps-momentum-us\\\\test_output.txt'' -ErrorAction SilentlyContinue; Write-Output ''Cleaned up test files''\")",
      "Bash(git -C \"C:\\\\dev\\\\claude-code\\\\eps-momentum-us\" status)",
      "Bash(git -C \"C:\\\\dev\\\\claude-code\\\\eps-momentum-us\" diff --stat)",
      "Bash(git -C \"C:\\\\dev\\\\claude-code\\\\eps-momentum-us\" log --oneline -5)",
      "Bash(git -C \"C:\\\\dev\\\\claude-code\\\\eps-momentum-us\" add README.md SESSION_HANDOFF.md daily_runner.py .claude/settings.local.json)",
      "Bash(git -C \"C:\\\\dev\\\\claude-code\\\\eps-momentum-us\" rm --cached test_ai_prompt.py test_output.txt)",
      "Bash(git -C \"C:\\\\dev\\\\claude-code\\\\eps-momentum-us\" add test_ai_prompt.py test_output.txt)",
      "Bash(git -C \"C:\\\\dev\\\\claude-code\\\\eps-momentum-us\" add -A)",
      "Bash(git -C \"C:\\\\dev\\\\claude-code\\\\eps-momentum-us\" commit -m \"$\\(cat <<''EOF''\nrefactor: AI 뉴스 스캐너 → AI 브리핑 전환 \\(v9.2\\)\n\n- AI 역할 변경: 리스크 소거법 → 데이터 분석 브리핑\n  검색은 코드가\\(yfinance\\), 분석은 AI가\\(Gemini\\) 원칙\n- 📰 시장 동향: Google Search 1회로 시장 요약\n- 📊 매수 후보 분석: results_df에서 직접 구조화 데이터 전달\n- 📅 어닝: yfinance stock.calendar 직접 조회 \\(할루시네이션 방지\\)\n- 청크 분할 버그 수정: split_point<=0 방어, 빈 청크 필터링\n- temperature 0.2→0.3, 빈 응답 재시도 로직 추가\n- 테스트 스크립트 정리 \\(test_ai_prompt.py, test_output.txt 삭제\\)\n\nCo-Authored-By: Claude Opus 4.6 <noreply@anthropic.com>\nEOF\n\\)\")",
      "Bash(git -C \"C:\\\\dev\\\\claude-code\\\\eps-momentum-us\" push origin master)",
      "Bash(powershell.exe -Command \"& ''C:\\\\Users\\\\jkw88\\\\miniconda3\\\\envs\\\\volumequant\\\\python.exe'' ''C:\\\\dev\\\\claude-code\\\\eps-momentum-us\\\\daily_runner.py''\")",
      "Bash(git -C \"C:\\\\dev\\\\claude-code\\\\eps-momentum-us\" add daily_runner.py README.md)",
      "Bash(git -C \"C:\\\\dev\\\\claude-code\\\\eps-momentum-us\" commit -m \"$\\(cat <<''EOF''\nfix: GitHub Actions시 Part1/Part2/AI브리핑 채널+개인봇 동시 발송\n\nCo-Authored-By: Claude Opus 4.6 <noreply@anthropic.com>\nEOF\n\\)\")",
      "Bash(git -C \"C:\\\\dev\\\\claude-code\\\\eps-momentum-us\" pull origin master)",
      "Bash(git -C \"C:\\\\dev\\\\claude-code\\\\eps-momentum-us\" stash)",
      "Bash(git -C \"C:\\\\dev\\\\claude-code\\\\eps-momentum-us\" stash pop)",
      "Bash(git -C \"C:\\\\dev\\\\claude-code\\\\eps-momentum-us\" log --oneline -10)",
      "Bash(git -C \"C:\\\\dev\\\\claude-code\\\\eps-momentum-us\" log --oneline 1d246ea..49ae477)",
      "Bash(git -C \"C:\\\\dev\\\\claude-code\\\\eps-momentum-us\" add eps_momentum_system.py daily_runner.py .claude/settings.local.json)",
      "Bash(git -C \"C:\\\\dev\\\\claude-code\\\\eps-momentum-us\" commit -m \"$\\(cat <<''EOF''\nrefactor: 신호등→날씨 아이콘 전환 + 포트폴리오 비중 단순화\n\n- 6단계 신호등\\(🟩🟢🔵🟡🔴🟥\\) → 5단계 날씨\\(☀️🌤️☁️🌧️⛈️\\)\n  임계값: >10%, 2~10%, -2~2%, -10~-2%, <-10%\n- 모바일 가독성 개선: 각 아이콘 형태가 완전히 달라 구분 용이\n- 포트폴리오 비중: 상한/하한 제거, adj_score 단순 비례 배분\n\nCo-Authored-By: Claude Opus 4.6 <noreply@anthropic.com>\nEOF\n\\)\")",
      "Bash(git -C \"C:\\\\dev\\\\claude-code\\\\eps-momentum-us\" add README.md SESSION_HANDOFF.md)",
      "Bash(git -C \"C:\\\\dev\\\\claude-code\\\\eps-momentum-us\" commit -m \"$\\(cat <<''EOF''\ndocs: v15 날씨 아이콘 전환 + 포트폴리오 비중 단순화 문서 반영\n\n- README: 추세 표시 섹션 날씨 아이콘으로 업데이트\n- SESSION_HANDOFF: Phase 11 추가 \\(데이터 분포 분석, 임계값 선정 근거\\)\n\nCo-Authored-By: Claude Opus 4.6 <noreply@anthropic.com>\nEOF\n\\)\")",
      "Bash(git -C \"C:\\\\dev\\\\claude-code\\\\eps-momentum-us\" diff --stat README.md SESSION_HANDOFF.md)",
      "Bash(git -C \"C:\\\\dev\\\\claude-code\\\\eps-momentum-us\" log --oneline -3)",
      "Bash(git -C \"C:\\\\dev\\\\claude-code\\\\eps-momentum-us\" show --stat 797739f)",
      "Bash(cd:*)",
      "Bash(gh workflow run:*)",
      "Bash(\"/c/Program Files/GitHub CLI/gh.exe\" workflow run \"Test Private Bot Only\")",
      "Bash(\"/c/Program Files/GitHub CLI/gh.exe\" run list --workflow=\"Test Private Bot Only\" --limit 3)",
      "Bash(\"/c/Program Files/GitHub CLI/gh.exe\" run watch 21863164333)",
      "Bash(\"/c/Program Files/GitHub CLI/gh.exe\" run view 21863164333 --log)",
      "Bash(git -C \"C:\\\\dev\\\\claude-code\\\\eps-momentum-us\" diff daily_runner.py --stat)",
      "Bash(git -C \"C:\\\\dev\\\\claude-code\\\\eps-momentum-us\" diff --stat daily_runner.py)",
      "Bash(git -C \"C:\\\\dev\\\\claude-code\\\\eps-momentum-us\" add daily_runner.py SESSION_HANDOFF.md)",
      "Bash(git -C \"C:\\\\dev\\\\claude-code\\\\eps-momentum-us\" diff --cached --stat)",
      "Bash(gh run list:*)",
      "Bash(gh run view:*)",
      "Bash(gh run watch:*)",
      "Bash(wc:*)",
      "Bash(ls:*)",
      "Bash(gh workflow:*)",
      "Bash(git -C \"C:\\\\dev\\\\claude-code\\\\eps-momentum-us\" commit -m \"$\\(cat <<''EOF''\nv31: final_action Q×VIX×q_days 14케이스 + UI 개선\n\n- final_action: 8→14케이스 \\(Q3 60일, Q4 20/60일 기준 분리\\)\n- Q4 후기\\(>60d\\)+VIX ok: \"바닥권 접근, 분할 매수\" \\(EDA 기반\\)\n- Q4 초기\\(≤20d\\): \"급매도 금물, 관망\" \\(초기 반등 가능\\)\n- Q1+VIX warn: \"반등 기회, 적극 투자\" \\(VIX40+ 역설\\)\n- [1/4] HY+VIX+신호등 1블록 압축, q_days \"N일째\" 표시\n- [2/4] 주도업종 이동 \\([1/4]→[2/4]\\)\n- Death List 경보 q_days 반영 \\(Q4 후기=바닥권\\)\n- 가이드: 겨울 3단계 \\(초기=관망/후기=바닥접근매수기회\\)\n- AI 프롬프트 2곳 q_days 포함\n- fetch_hy_quadrant\\(\\) cash_pct 제거, action 단순화\n\nCo-Authored-By: Claude Opus 4.6 <noreply@anthropic.com>\nEOF\n\\)\")",
      "Bash(git -C \"C:\\\\dev\\\\claude-code\\\\eps-momentum-us\" push)",
      "Bash(while read hash msg)",
      "Bash(do echo \"=== $msg ===\")",
      "Bash(git init:*)",
      "Bash(gh repo create:*)",
      "Bash(npm install)",
      "Bash(pip install:*)",
      "Bash(npx tsc:*)",
      "Bash(curl:*)",
      "Bash(taskkill:*)",
      "Bash(npm run build:*)",
      "Bash(pkill:*)",
      "Bash(netstat:*)",
      "Bash(# Force kill old PID taskkill //F //PID 19844 sleep 1 # Verify curl -s http://localhost:8000/api/screening/2026-02-20)",
      "Bash(# Find ALL processes on port 8000 netstat -ano)",
      "Bash(# Check if 8001 has anything else netstat -ano)",
      "Bash(git checkout:*)",
      "Bash(PYTHONIOENCODING=utf-8 cat:*)",
      "Bash(PYTHONIOENCODING=utf-8 python:*)",
      "Bash(cat << 'PYEOF' > /c/dev/claude-code/eps-momentum-us/analyze_thresholds.py\nimport sys\nsys.stdout.reconfigure\\(encoding='utf-8'\\)\nimport sqlite3\nfrom collections import defaultdict\n\nDB_PATH = r\"C:\\\\dev\\\\claude-code\\\\eps-momentum-us\\\\eps_momentum_data.db\"\n\nPENALTY = 50\n\n# Thresholds to test\nTHRESHOLDS = {\n    \"Current\":  {\"RANK\": 3, \"GAP\": 3.0, \"SCORE\": 1.5},\n    \"Alt A\":    {\"RANK\": 3, \"GAP\": 2.0, \"SCORE\": 1.0},\n    \"Alt B\":    {\"RANK\": 4, \"GAP\": 3.0, \"SCORE\": 1.5},\n    \"Alt C\":    {\"RANK\": 3, \"GAP\": 2.5, \"SCORE\": 1.0},\n}\n\ndef assign_tag\\(rank_chg, gap_delta, score_delta, rank_thresh, gap_thresh, score_thresh\\):\n    \"\"\"Assign a tag based on rank change and factor deltas.\"\"\"\n    abs_rc = abs\\(rank_chg\\)\n    if abs_rc < rank_thresh:\n        return None  # no tag needed \\(rank barely changed\\)\n\n    direction = \"up\" if rank_chg < 0 else \"down\"  # lower rank = better\n\n    # Determine the factor\n    abs_gap = abs\\(gap_delta\\) if gap_delta is not None else 0\n    abs_score = abs\\(score_delta\\) if score_delta is not None else 0\n\n    tag = None\n    if abs_gap >= gap_thresh and abs_score >= score_thresh:\n        # Both factors moved significantly\n        if direction == \"up\":\n            tag = \"📈GAP+EPS\"\n        else:\n            tag = \"📉GAP+EPS\"\n    elif abs_gap >= gap_thresh:\n        if direction == \"up\":\n            tag = \"📈괴리축소\"\n        else:\n            tag = \"📉괴리확대\"\n    elif abs_score >= score_thresh:\n        if direction == \"up\":\n            tag = \"📈EPS가속\"\n        else:\n            tag = \"📉EPS감속\"\n    else:\n        tag = \"🔄상대변동\"\n\n    return tag\n\n\ndef main\\(\\):\n    conn = sqlite3.connect\\(DB_PATH\\)\n    conn.row_factory = sqlite3.Row\n    cur = conn.cursor\\(\\)\n\n    # 1. Get all dates with composite_rank data\n    cur.execute\\(\"SELECT DISTINCT date FROM ntm_screening WHERE composite_rank IS NOT NULL ORDER BY date\"\\)\n    dates = [row[0] for row in cur.fetchall\\(\\)]\n    print\\(f\"Dates with composite_rank data: {len\\(dates\\)}\"\\)\n    for d in dates:\n        print\\(f\"  {d}\"\\)\n    print\\(\\)\n\n    if len\\(dates\\) < 2:\n        print\\(\"Not enough dates for analysis \\(need at least 2\\).\"\\)\n        return\n\n    # 2. For each date \\(T0\\), build triplet analysis\n    all_records = []\n\n    for i, t0 in enumerate\\(dates\\):\n        t1 = dates[i - 1] if i >= 1 else None\n        t2 = dates[i - 2] if i >= 2 else None\n\n        # Get stocks with part2_rank on T0 \\(the eligible Top 30\\)\n        cur.execute\\(\"\"\"\n            SELECT ticker, composite_rank, adj_gap, adj_score, part2_rank\n            FROM ntm_screening\n            WHERE date = ? AND part2_rank IS NOT NULL\n            ORDER BY part2_rank\n        \"\"\", \\(t0,\\)\\)\n        t0_stocks = {row['ticker']: dict\\(row\\) for row in cur.fetchall\\(\\)}\n\n        if not t0_stocks:\n            continue\n\n        # Get T1 data\n        t1_data = {}\n        if t1:\n            cur.execute\\(\"\"\"\n                SELECT ticker, composite_rank, adj_gap, adj_score\n                FROM ntm_screening\n                WHERE date = ? AND composite_rank IS NOT NULL\n            \"\"\", \\(t1,\\)\\)\n            t1_data = {row['ticker']: dict\\(row\\) for row in cur.fetchall\\(\\)}\n\n        # Get T2 data\n        t2_data = {}\n        if t2:\n            cur.execute\\(\"\"\"\n                SELECT ticker, composite_rank, adj_gap, adj_score\n                FROM ntm_screening\n                WHERE date = ? AND composite_rank IS NOT NULL\n            \"\"\", \\(t2,\\)\\)\n            t2_data = {row['ticker']: dict\\(row\\) for row in cur.fetchall\\(\\)}\n\n        for ticker, t0_info in t0_stocks.items\\(\\):\n            r0 = t0_info['composite_rank']\n            r1 = t1_data.get\\(ticker, {}\\).get\\('composite_rank', PENALTY\\) if t1 else None\n            r2 = t2_data.get\\(ticker, {}\\).get\\('composite_rank', PENALTY\\) if t2 else None\n\n            # Determine trajectory type\n            has_t2 = t2 is not None\n            if has_t2:\n                ref_rank = r2\n                ref_date = t2\n                trajectory = \"3-day\"\n            elif t1 is not None:\n                ref_rank = r1\n                ref_date = t1\n                trajectory = \"2-day\"\n            else:\n                continue  # first date, skip\n\n            rank_chg = r0 - ref_rank\n\n            # Get adj_gap and adj_score deltas\n            gap_t0 = t0_info.get\\('adj_gap'\\)\n            score_t0 = t0_info.get\\('adj_score'\\)\n\n            if has_t2:\n                ref_info = t2_data.get\\(ticker, {}\\)\n            else:\n                ref_info = t1_data.get\\(ticker, {}\\)\n\n            gap_ref = ref_info.get\\('adj_gap'\\)\n            score_ref = ref_info.get\\('adj_score'\\)\n\n            gap_delta = None\n            if gap_t0 is not None and gap_ref is not None:\n                gap_delta = gap_t0 - gap_ref\n\n            score_delta = None\n            if score_t0 is not None and score_ref is not None:\n                score_delta = score_t0 - score_ref\n\n            # Assign tags for each threshold set\n            tags = {}\n            for name, th in THRESHOLDS.items\\(\\):\n                tags[name] = assign_tag\\(rank_chg, gap_delta, score_delta,\n                                        th[\"RANK\"], th[\"GAP\"], th[\"SCORE\"]\\)\n\n            all_records.append\\({\n                \"date\": t0,\n                \"ticker\": ticker,\n                \"part2_rank\": t0_info['part2_rank'],\n                \"r2\": r2,\n                \"r1\": r1,\n                \"r0\": r0,\n                \"rank_chg\": rank_chg,\n                \"trajectory\": trajectory,\n                \"gap_t0\": gap_t0,\n                \"gap_ref\": gap_ref,\n                \"gap_delta\": gap_delta,\n                \"score_t0\": score_t0,\n                \"score_ref\": score_ref,\n                \"score_delta\": score_delta,\n                \"tags\": tags,\n            }\\)\n\n    # 3. Print detailed table\n    print\\(\"=\" * 160\\)\n    print\\(\"DETAILED RANK CHANGE ANALYSIS — ALL DATES\"\\)\n    print\\(\"=\" * 160\\)\n    header = f\"{'Date':<12} {'Ticker':<7} {'P2Rk':>4} {'r2':>4} {'r1':>4} {'r0':>4} {'rChg':>5} {'Traj':<5} {'gapD':>7} {'scrD':>7} {'Current':<14} {'Alt A':<14} {'Alt B':<14} {'Alt C':<14}\"\n    print\\(header\\)\n    print\\(\"-\" * 160\\)\n\n    current_date = None\n    for rec in all_records:\n        if rec['date'] != current_date:\n            if current_date is not None:\n                print\\(\"-\" * 160\\)\n            current_date = rec['date']\n\n        r2_str = f\"{rec['r2']}\" if rec['r2'] is not None else \"-\"\n        r1_str = f\"{rec['r1']}\" if rec['r1'] is not None else \"-\"\n        gap_d_str = f\"{rec['gap_delta']:+.1f}\" if rec['gap_delta'] is not None else \"N/A\"\n        score_d_str = f\"{rec['score_delta']:+.1f}\" if rec['score_delta'] is not None else \"N/A\"\n\n        tag_cur = rec['tags'].get\\('Current', '-'\\) or '-'\n        tag_a = rec['tags'].get\\('Alt A', '-'\\) or '-'\n        tag_b = rec['tags'].get\\('Alt B', '-'\\) or '-'\n        tag_c = rec['tags'].get\\('Alt C', '-'\\) or '-'\n\n        print\\(f\"{rec['date']:<12} {rec['ticker']:<7} {rec['part2_rank']:>4} {r2_str:>4} {r1_str:>4} {rec['r0']:>4} {rec['rank_chg']:>+5} {rec['trajectory']:<5} {gap_d_str:>7} {score_d_str:>7} {tag_cur:<14} {tag_a:<14} {tag_b:<14} {tag_c:<14}\"\\)\n\n    print\\(\"=\" * 160\\)\n    print\\(\\)\n\n    # 4. Summary Statistics\n    print\\(\"=\" * 80\\)\n    print\\(\"SUMMARY STATISTICS\"\\)\n    print\\(\"=\" * 80\\)\n\n    # Distribution of |rank_chg|\n    rank_buckets = defaultdict\\(int\\)\n    for rec in all_records:\n        arc = abs\\(rec['rank_chg']\\)\n        if arc == 0: rank_buckets['0'] += 1\n        elif arc == 1: rank_buckets['1'] += 1\n        elif arc == 2: rank_buckets['2'] += 1\n        elif arc == 3: rank_buckets['3'] += 1\n        elif arc == 4: rank_buckets['4'] += 1\n        else: rank_buckets['5+'] += 1\n\n    print\\(\"\\\\n|rank_chg| Distribution:\"\\)\n    print\\(f\"  {'Bucket':<8} {'Count':>6} {'%':>7}\"\\)\n    total = len\\(all_records\\)\n    for bucket in ['0', '1', '2', '3', '4', '5+']:\n        cnt = rank_buckets[bucket]\n        pct = cnt / total * 100 if total else 0\n        bar = '#' * int\\(pct\\)\n        print\\(f\"  {bucket:<8} {cnt:>6} {pct:>6.1f}% {bar}\"\\)\n\n    # Distribution of |gap_delta|\n    gap_buckets = defaultdict\\(int\\)\n    gap_count = 0\n    for rec in all_records:\n        if rec['gap_delta'] is not None:\n            agd = abs\\(rec['gap_delta']\\)\n            gap_count += 1\n            if agd < 1: gap_buckets['0-1'] += 1\n            elif agd < 2: gap_buckets['1-2'] += 1\n            elif agd < 3: gap_buckets['2-3'] += 1\n            elif agd < 5: gap_buckets['3-5'] += 1\n            elif agd < 10: gap_buckets['5-10'] += 1\n            else: gap_buckets['10+'] += 1\n\n    print\\(f\"\\\\n|gap_delta| Distribution \\(N={gap_count}\\):\"\\)\n    print\\(f\"  {'Bucket':<8} {'Count':>6} {'%':>7}\"\\)\n    for bucket in ['0-1', '1-2', '2-3', '3-5', '5-10', '10+']:\n        cnt = gap_buckets[bucket]\n        pct = cnt / gap_count * 100 if gap_count else 0\n        bar = '#' * int\\(pct\\)\n        print\\(f\"  {bucket:<8} {cnt:>6} {pct:>6.1f}% {bar}\"\\)\n\n    # Distribution of |score_delta|\n    score_buckets = defaultdict\\(int\\)\n    score_count = 0\n    for rec in all_records:\n        if rec['score_delta'] is not None:\n            asd = abs\\(rec['score_delta']\\)\n            score_count += 1\n            if asd < 0.5: score_buckets['0-0.5'] += 1\n            elif asd < 1.0: score_buckets['0.5-1.0'] += 1\n            elif asd < 1.5: score_buckets['1.0-1.5'] += 1\n            elif asd < 2.0: score_buckets['1.5-2.0'] += 1\n            elif asd < 3.0: score_buckets['2.0-3.0'] += 1\n            else: score_buckets['3.0+'] += 1\n\n    print\\(f\"\\\\n|score_delta| Distribution \\(N={score_count}\\):\"\\)\n    print\\(f\"  {'Bucket':<8} {'Count':>6} {'%':>7}\"\\)\n    for bucket in ['0-0.5', '0.5-1.0', '1.0-1.5', '1.5-2.0', '2.0-3.0', '3.0+']:\n        cnt = score_buckets[bucket]\n        pct = cnt / score_count * 100 if score_count else 0\n        bar = '#' * int\\(pct\\)\n        print\\(f\"  {bucket:<8} {cnt:>6} {pct:>6.1f}% {bar}\"\\)\n\n    # For stocks with |rank_chg| >= 3: specific tag vs relative tag\n    print\\(f\"\\\\nTag breakdown for stocks with |rank_chg| >= RANK_THRESHOLD \\(Current=3\\):\"\\)\n    eligible = [r for r in all_records if abs\\(r['rank_chg']\\) >= 3]\n    tag_counts = defaultdict\\(int\\)\n    for r in eligible:\n        t = r['tags']['Current'] or 'None'\n        tag_counts[t] += 1\n    print\\(f\"  Total eligible: {len\\(eligible\\)}\"\\)\n    for tag, cnt in sorted\\(tag_counts.items\\(\\), key=lambda x: -x[1]\\):\n        pct = cnt / len\\(eligible\\) * 100 if eligible else 0\n        print\\(f\"  {tag:<20} {cnt:>4} \\({pct:5.1f}%\\)\"\\)\n\n    # Tag coverage rate per date\n    print\\(f\"\\\\nTag coverage rate per date \\(Current thresholds\\):\"\\)\n    print\\(f\"  {'Date':<12} {'Total':>5} {'Tagged':>6} {'NoTag':>5} {'Coverage':>8}\"\\)\n    dates_in_records = sorted\\(set\\(r['date'] for r in all_records\\)\\)\n    total_tagged_all = 0\n    total_eligible_all = 0\n    for d in dates_in_records:\n        date_recs = [r for r in all_records if r['date'] == d]\n        tagged = sum\\(1 for r in date_recs if r['tags']['Current'] is not None\\)\n        notag = len\\(date_recs\\) - tagged\n        cov = tagged / len\\(date_recs\\) * 100 if date_recs else 0\n        total_tagged_all += tagged\n        total_eligible_all += len\\(date_recs\\)\n        print\\(f\"  {d:<12} {len\\(date_recs\\):>5} {tagged:>6} {notag:>5} {cov:>7.1f}%\"\\)\n    if total_eligible_all:\n        print\\(f\"  {'OVERALL':<12} {total_eligible_all:>5} {total_tagged_all:>6} {total_eligible_all - total_tagged_all:>5} {total_tagged_all/total_eligible_all*100:>7.1f}%\"\\)\n\n    # 5. Simulate alternative thresholds\n    print\\(\\)\n    print\\(\"=\" * 80\\)\n    print\\(\"THRESHOLD COMPARISON\"\\)\n    print\\(\"=\" * 80\\)\n    print\\(f\"\\\\n{'Threshold':<12} {'RANK':>4} {'GAP':>5} {'SCORE':>5} | {'Tagged':>6} {'Specific':>8} {'Relative':>8} {'NoTag':>5} | {'Spec%':>6} {'Rel%':>6} {'Cov%':>6}\"\\)\n    print\\(\"-\" * 95\\)\n\n    for name, th in THRESHOLDS.items\\(\\):\n        tagged = 0\n        specific = 0\n        relative = 0\n        notag = 0\n        for rec in all_records:\n            tag = assign_tag\\(rec['rank_chg'], rec['gap_delta'], rec['score_delta'],\n                             th[\"RANK\"], th[\"GAP\"], th[\"SCORE\"]\\)\n            if tag is None:\n                notag += 1\n            elif tag == \"🔄상대변동\":\n                relative += 1\n                tagged += 1\n            else:\n                specific += 1\n                tagged += 1\n\n        spec_pct = specific / tagged * 100 if tagged else 0\n        rel_pct = relative / tagged * 100 if tagged else 0\n        cov_pct = tagged / total * 100 if total else 0\n\n        print\\(f\"{name:<12} {th['RANK']:>4} {th['GAP']:>5.1f} {th['SCORE']:>5.1f} | {tagged:>6} {specific:>8} {relative:>8} {notag:>5} | {spec_pct:>5.1f}% {rel_pct:>5.1f}% {cov_pct:>5.1f}%\"\\)\n\n    # Per-date breakdown for each threshold\n    print\\(f\"\\\\nPer-date coverage by threshold:\"\\)\n    print\\(f\"  {'Date':<12} {'N':>3}\", end=\"\"\\)\n    for name in THRESHOLDS:\n        print\\(f\" | {name:^22}\", end=\"\"\\)\n    print\\(\\)\n    print\\(f\"  {'':12} {'':3}\", end=\"\"\\)\n    for _ in THRESHOLDS:\n        print\\(f\" | {'Tag':>4} {'Spec':>4} {'Rel':>4} {'Cov%':>6}\", end=\"\"\\)\n    print\\(\\)\n    print\\(\"  \" + \"-\" * \\(15 + len\\(THRESHOLDS\\) * 25\\)\\)\n\n    for d in dates_in_records:\n        date_recs = [r for r in all_records if r['date'] == d]\n        n = len\\(date_recs\\)\n        print\\(f\"  {d:<12} {n:>3}\", end=\"\"\\)\n        for name, th in THRESHOLDS.items\\(\\):\n            tagged = 0\n            specific = 0\n            relative = 0\n            for rec in date_recs:\n                tag = assign_tag\\(rec['rank_chg'], rec['gap_delta'], rec['score_delta'],\n                                 th[\"RANK\"], th[\"GAP\"], th[\"SCORE\"]\\)\n                if tag is not None:\n                    tagged += 1\n                    if tag == \"🔄상대변동\":\n                        relative += 1\n                    else:\n                        specific += 1\n            cov = tagged / n * 100 if n else 0\n            print\\(f\" | {tagged:>4} {specific:>4} {relative:>4} {cov:>5.1f}%\", end=\"\"\\)\n        print\\(\\)\n\n    conn.close\\(\\)\n    print\\(\"\\\\nDone.\"\\)\n\n\nif __name__ == \"__main__\":\n    main\\(\\)\nPYEOF\necho \"Script written.\")",
      "Bash(cat > \"C:/dev/claude-code/eps-momentum-us/backtest_top5.py\" << 'PYEOF'\n\"\"\"\nEPS Momentum Backtest: Top 5 Entry / Top 30 Hold Strategy\n==========================================================\n- Enter when stock first appears in Top 5 \\(part2_rank 1-5\\)\n- Hold as long as stock stays in Top 30 \\(part2_rank 1-30\\)\n- Exit when stock leaves Top 30\n- Equal weight allocation on entry\n- Uses actual prices from ntm_screening table\n\"\"\"\n\nimport sqlite3\nimport sys\nfrom collections import defaultdict\n\nsys.stdout.reconfigure\\(encoding='utf-8'\\)\n\nDB_PATH = r\"C:\\\\dev\\\\claude-code\\\\eps-momentum-us\\\\eps_momentum_data.db\"\n\ndef run_backtest\\(\\):\n    conn = sqlite3.connect\\(DB_PATH\\)\n    conn.row_factory = sqlite3.Row\n    cur = conn.cursor\\(\\)\n\n    # 1. Get all dates with part2_rank data\n    cur.execute\\(\"\"\"\n        SELECT DISTINCT date FROM ntm_screening \n        WHERE part2_rank IS NOT NULL \n        ORDER BY date\n    \"\"\"\\)\n    dates = [r['date'] for r in cur.fetchall\\(\\)]\n    print\\(f\"백테스트 기간: {dates[0]} ~ {dates[-1]} \\({len\\(dates\\)}일\\)\"\\)\n    print\\(\"=\" * 100\\)\n\n    # 2. Load all data into memory: date -> {ticker: {rank, price}}\n    daily_data = {}\n    for d in dates:\n        cur.execute\\(\"\"\"\n            SELECT ticker, part2_rank, price \n            FROM ntm_screening \n            WHERE date = ? AND part2_rank IS NOT NULL\n            ORDER BY part2_rank\n        \"\"\", \\(d,\\)\\)\n        daily_data[d] = {}\n        for row in cur.fetchall\\(\\):\n            daily_data[d][row['ticker']] = {\n                'rank': row['part2_rank'],\n                'price': row['price']\n            }\n\n    # 3. Run the backtest\n    portfolio = {}  # {ticker: {entry_date, entry_price, shares, entry_day_idx}}\n    initial_capital = 100000.0\n    cash = initial_capital\n    trade_log = []\n    daily_snapshots = []\n    initial_top5 = {}\n\n    for day_idx, date in enumerate\\(dates\\):\n        data = daily_data[date]\n        top5_tickers = {t for t, d in data.items\\(\\) if d['rank'] <= 5}\n        top30_tickers = {t for t, d in data.items\\(\\) if d['rank'] <= 30}\n\n        entries_log = []\n        exits_log = []\n\n        # EXIT: Remove stocks that left Top 30\n        tickers_to_exit = [t for t in portfolio if t not in top30_tickers]\n\n        for ticker in tickers_to_exit:\n            pos = portfolio[ticker]\n            # Get exit price: try today's full data first, then previous day\n            cur.execute\\(\"SELECT price FROM ntm_screening WHERE date = ? AND ticker = ?\", \\(date, ticker\\)\\)\n            row = cur.fetchone\\(\\)\n            if row and row['price']:\n                exit_price = row['price']\n            else:\n                prev_date = dates[day_idx - 1] if day_idx > 0 else date\n                cur.execute\\(\"SELECT price FROM ntm_screening WHERE date = ? AND ticker = ?\", \\(prev_date, ticker\\)\\)\n                row = cur.fetchone\\(\\)\n                exit_price = row['price'] if row else pos['entry_price']\n\n            return_pct = \\(exit_price / pos['entry_price'] - 1\\) * 100\n            proceeds = pos['shares'] * exit_price\n            cash += proceeds\n\n            trade_log.append\\({\n                'ticker': ticker, 'action': 'EXIT', 'date': date,\n                'price': exit_price, 'entry_date': pos['entry_date'],\n                'entry_price': pos['entry_price'], 'return_pct': return_pct,\n                'proceeds': proceeds, 'holding_days': day_idx - pos['entry_day_idx']\n            }\\)\n            exits_log.append\\(f\"  EXIT  {ticker}: ${pos['entry_price']:.2f} -> ${exit_price:.2f} \\({return_pct:+.2f}%\\)\"\\)\n            del portfolio[ticker]\n\n        # ENTRY: Buy new stocks that entered Top 5\n        new_entries = top5_tickers - set\\(portfolio.keys\\(\\)\\)\n        if day_idx == 0:\n            # First day: buy all Top 5 equally\n            allocation_per_stock = initial_capital / len\\(top5_tickers\\) if top5_tickers else 0\n            for ticker in sorted\\(top5_tickers\\):\n                if ticker in data:\n                    price = data[ticker]['price']\n                    if price > 0:\n                        shares = allocation_per_stock / price\n                        cash -= shares * price\n                        portfolio[ticker] = {\n                            'entry_date': date, 'entry_price': price,\n                            'shares': shares, 'entry_day_idx': day_idx\n                        }\n                        entries_log.append\\(f\"  ENTER {ticker}: ${price:.2f} \\(rank #{data[ticker]['rank']}, ${shares*price:,.0f} allocated\\)\"\\)\n                        initial_top5[ticker] = {'entry_price': price, 'shares': shares}\n        elif new_entries:\n            # Subsequent days: allocate available cash equally among new entries\n            allocation_per_stock = cash / len\\(new_entries\\) if new_entries else 0\n            allocation_per_stock = min\\(allocation_per_stock, initial_capital * 0.25\\)\n\n            for ticker in sorted\\(new_entries\\):\n                if ticker in data:\n                    price = data[ticker]['price']\n                    if price > 0 and cash >= 100:\n                        actual_alloc = min\\(allocation_per_stock, cash\\)\n                        shares = actual_alloc / price\n                        cash -= shares * price\n                        portfolio[ticker] = {\n                            'entry_date': date, 'entry_price': price,\n                            'shares': shares, 'entry_day_idx': day_idx\n                        }\n                        entries_log.append\\(f\"  ENTER {ticker}: ${price:.2f} \\(rank #{data[ticker]['rank']}, ${shares*price:,.0f} allocated\\)\"\\)\n\n        # Daily Portfolio Valuation\n        portfolio_value = cash\n        position_details = []\n        for ticker, pos in sorted\\(portfolio.items\\(\\)\\):\n            current_price = data[ticker]['price'] if ticker in data else pos['entry_price']\n            pos_value = pos['shares'] * current_price\n            pos_return = \\(current_price / pos['entry_price'] - 1\\) * 100\n            portfolio_value += pos_value\n            rank_str = f\"#{data[ticker]['rank']}\" if ticker in data else \"N/A\"\n            position_details.append\\(\n                f\"    {ticker:>6}: ${current_price:>8.2f} \\(rank {rank_str:>4}, {pos_return:>+7.2f}%, ${pos_value:>10,.0f}\\)\"\n            \\)\n\n        total_return = \\(portfolio_value / initial_capital - 1\\) * 100\n        daily_snapshots.append\\({\n            'date': date, 'portfolio_value': portfolio_value,\n            'cash': cash, 'num_positions': len\\(portfolio\\), 'total_return': total_return\n        }\\)\n\n        # Print daily summary\n        print\\(f\"\\\\n{'='*80}\"\\)\n        print\\(f\"  {date} | Portfolio: ${portfolio_value:,.2f} \\({total_return:+.2f}%\\) | Cash: ${cash:,.0f} | Positions: {len\\(portfolio\\)}\"\\)\n        print\\(f\"{'='*80}\"\\)\n        if exits_log:\n            for e in exits_log: print\\(e\\)\n        if entries_log:\n            for e in entries_log: print\\(e\\)\n        if position_details:\n            print\\(\"  -- Holdings --\"\\)\n            for p in position_details: print\\(p\\)\n\n        top5_display = []\n        for t in sorted\\(data.keys\\(\\), key=lambda x: data[x]['rank']\\):\n            if data[t]['rank'] <= 5:\n                flag = \"[HOLD]\" if t in portfolio else \"[----]\"\n                top5_display.append\\(f\"{flag} {t}\\(#{data[t]['rank']}\\)\"\\)\n        print\\(f\"  Today's Top 5: {', '.join\\(top5_display\\)}\"\\)\n\n    # ── FINAL SUMMARY ──\n    print\\(\"\\\\n\" + \"=\" * 100\\)\n    print\\(\"  BACKTEST RESULTS SUMMARY\"\\)\n    print\\(\"=\" * 100\\)\n\n    final_value = daily_snapshots[-1]['portfolio_value']\n    total_return = \\(final_value / initial_capital - 1\\) * 100\n    print\\(f\"\\\\n  Initial Capital:  ${initial_capital:,.2f}\"\\)\n    print\\(f\"  Final Value:      ${final_value:,.2f}\"\\)\n    print\\(f\"  Total Return:     {total_return:+.2f}%\"\\)\n    print\\(f\"  Trading Days:     {len\\(dates\\)}\"\\)\n\n    # Trade Summary\n    print\\(f\"\\\\n  -- Completed Trades --\"\\)\n    completed = [t for t in trade_log if t['action'] == 'EXIT']\n    if completed:\n        print\\(f\"  Total: {len\\(completed\\)} trades\"\\)\n        for t in completed:\n            print\\(f\"    {t['ticker']:>6}: {t['entry_date']} -> {t['date']} | \"\n                  f\"${t['entry_price']:>8.2f} -> ${t['price']:>8.2f} | \"\n                  f\"{t['return_pct']:>+7.2f}% | {t['holding_days']}d held\"\\)\n        avg_ret = sum\\(t['return_pct'] for t in completed\\) / len\\(completed\\)\n        winners = sum\\(1 for t in completed if t['return_pct'] > 0\\)\n        losers = sum\\(1 for t in completed if t['return_pct'] <= 0\\)\n        print\\(f\"    Avg Return: {avg_ret:+.2f}%  |  Win Rate: {winners}/{len\\(completed\\)} \\({winners/len\\(completed\\)*100:.0f}%\\)\"\\)\n    else:\n        print\\(\"    No completed trades \\(all positions still open\\)\"\\)\n\n    # Open Positions\n    print\\(f\"\\\\n  -- Open Positions \\(Unrealized\\) --\"\\)\n    last_date = dates[-1]\n    last_data = daily_data[last_date]\n    for ticker, pos in sorted\\(portfolio.items\\(\\)\\):\n        current_price = last_data[ticker]['price'] if ticker in last_data else pos['entry_price']\n        return_pct = \\(current_price / pos['entry_price'] - 1\\) * 100\n        pos_value = pos['shares'] * current_price\n        rank_str = f\"#{last_data[ticker]['rank']}\" if ticker in last_data else \"OUT\"\n        print\\(f\"    {ticker:>6}: {pos['entry_date']} -> {last_date} | \"\n              f\"${pos['entry_price']:>8.2f} -> ${current_price:>8.2f} | \"\n              f\"{return_pct:>+7.2f}% | rank {rank_str} | ${pos_value:>10,.0f}\"\\)\n\n    # Buy-and-Hold Comparison\n    print\\(f\"\\\\n  -- Buy & Hold Comparison \\(Initial Top 5 on {dates[0]}\\) --\"\\)\n    bh_value = 0\n    for ticker, pos in sorted\\(initial_top5.items\\(\\)\\):\n        current_price = None\n        for d in reversed\\(dates\\):\n            if ticker in daily_data[d]:\n                current_price = daily_data[d][ticker]['price']\n                break\n        if current_price is None:\n            current_price = pos['entry_price']\n        \n        bh_pos_value = pos['shares'] * current_price\n        bh_return = \\(current_price / pos['entry_price'] - 1\\) * 100\n        bh_value += bh_pos_value\n        print\\(f\"    {ticker:>6}: ${pos['entry_price']:>8.2f} -> ${current_price:>8.2f} \\({bh_return:>+7.2f}%\\) | ${bh_pos_value:>10,.0f}\"\\)\n\n    bh_total_return = \\(bh_value / initial_capital - 1\\) * 100\n    print\\(f\"\\\\n  {'Strategy Return:':>25} {total_return:+.2f}%  \\(${final_value:,.2f}\\)\"\\)\n    print\\(f\"  {'Buy & Hold Return:':>25} {bh_total_return:+.2f}%  \\(${bh_value:,.2f}\\)\"\\)\n    alpha = total_return - bh_total_return\n    print\\(f\"  {'Alpha \\(difference\\):':>25} {alpha:+.2f}%p\"\\)\n\n    # Daily Return Table\n    print\\(f\"\\\\n  -- Daily Portfolio Value --\"\\)\n    print\\(f\"  {'Date':>12} | {'Value':>14} | {'Return':>8} | {'#Pos':>4} | {'Cash':>10}\"\\)\n    print\\(f\"  {'-'*60}\"\\)\n    for snap in daily_snapshots:\n        print\\(f\"  {snap['date']:>12} | ${snap['portfolio_value']:>12,.2f} | \"\n              f\"{snap['total_return']:>+7.2f}% | {snap['num_positions']:>4} | ${snap['cash']:>8,.0f}\"\\)\n\n    # Turnover Analysis\n    print\\(f\"\\\\n  -- Top 5 Appearance Frequency --\"\\)\n    ticker_top5_count = defaultdict\\(int\\)\n    ticker_top30_count = defaultdict\\(int\\)\n    for d in dates:\n        for t, info in daily_data[d].items\\(\\):\n            if info['rank'] <= 5: ticker_top5_count[t] += 1\n            if info['rank'] <= 30: ticker_top30_count[t] += 1\n\n    print\\(f\"  {'Ticker':>8} | {'Top5 Days':>9} | {'Top30 Days':>10} | {'Top5 %':>6}\"\\)\n    print\\(f\"  {'-'*45}\"\\)\n    for t in sorted\\(ticker_top5_count.keys\\(\\), key=lambda x: -ticker_top5_count[x]\\):\n        ratio = ticker_top5_count[t] / len\\(dates\\) * 100\n        print\\(f\"  {t:>8} | {ticker_top5_count[t]:>7}d | {ticker_top30_count[t]:>8}d | {ratio:>5.0f}%\"\\)\n\n    # Unique tickers that passed through Top 30\n    all_top30 = set\\(\\)\n    for d in dates:\n        for t, info in daily_data[d].items\\(\\):\n            if info['rank'] <= 30: all_top30.add\\(t\\)\n    print\\(f\"\\\\n  Total unique tickers in Top 30 across period: {len\\(all_top30\\)}\"\\)\n    \n    conn.close\\(\\)\n\nif __name__ == \"__main__\":\n    run_backtest\\(\\)\nPYEO)",
      "WebFetch(domain:www.netguru.com)",
      "WebFetch(domain:www.xminstitute.com)",
      "WebFetch(domain:www.qualtrics.com)",
      "WebFetch(domain:www.axioshq.com)",
      "WebFetch(domain:t.me)",
      "WebFetch(domain:theaudiencers.com)",
      "WebFetch(domain:mergersandinquisitions.com)",
      "WebFetch(domain:theprudentspeculator.com)",
      "WebFetch(domain:financewalk.com)",
      "WebFetch(domain:analystsolutions.com)",
      "WebFetch(domain:traderhq.com)",
      "WebFetch(domain:www.newsletterexamples.co)",
      "WebFetch(domain:telegramfxcopier.io)",
      "WebFetch(domain:www.wallstreetprep.com)",
      "WebFetch(domain:easyvc.ai)",
      "WebFetch(domain:writingcooperative.com)",
      "WebFetch(domain:www.smashingmagazine.com)",
      "WebFetch(domain:brunch.co.kr)",
      "WebFetch(domain:www.hashtaginvesting.com)",
      "WebFetch(domain:www.failory.com)",
      "WebFetch(domain:formatsunpacked.storythings.com)"
    ]
  }
}
